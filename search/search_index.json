{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DEPy's documentation!","text":"<p>DEPy (aka SummarizedPy) is a Python library for bulk proteomics (and metabolomics) differential expression analysis. It was inspired by the R packages DEP and SummarizedExperiment. While Python containers for single-cell data exist within the scverse, tools for bulk -omics are less common, especially when it comes to proteomics.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Installation</li> <li>Usage</li> <li>API</li> <li>History</li> </ul>"},{"location":"#the-power-of-transcriptomics-unleashed-in-ms-based-omics","title":"The power of transcriptomics unleashed in MS-based -omics","text":"<p>Discovery proteomics shares a lot of statistical similarities with transcriptomics data once you have the final protein- or peptide-level intensity values (like a protein groups matrix from FragPipe or MaxQuant).</p> <p>More importantly, bulk -omics often face similar issues: variable sample quality, hidden batch effects, heteroscedasticity, and complex designs.</p> <p>Fortunately, several tools have been developed in the transcriptomics literature to address these very concerns! - limma - arrayWeights - vsn - sva</p> <p>Additionally, there are tools to address MS-based -omics issues, such as missing value imputation (e.g. ImputeLCMD). User-friendly software like Perseus come with easy filtering, transformation, imputation, and testing.</p> <p>Alas, none of this is available in Python...</p>"},{"location":"#why-depy","title":"Why DEPy","text":"<p>I spent my PhD leveraging R-based transcriptomics tools to power my proteomics and metabolomics analyses and tackle common issues associated with real-world, human and animal datasets.</p> <p>As I was migrating my workflows to Python (mainly for ML), I wanted to bring all of that Bioconductor goodness with me. So, I decided to package it - bringing the best of two worlds.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Leverage R packages like limma, arrayWeights, vsn, sva, ImputeLCMD</li> <li>Example proteomics dataset for testing and exploring</li> <li>SummarizedPy: A container for your data and metadata</li> <li>Numpy array-based storage for faster operations and integration with the greater Python stack</li> <li>I/O using Feather for memory efficiency and speed</li> <li>Isolated R environment run as a subprocess</li> <li>Easily scales to hundreds of samples and thousands of features</li> <li>History attribute logs every step and parameter setting</li> <li>High test coverage</li> </ul> <p>If that sounds interesting, please read on!</p>"},{"location":"HISTORY/","title":"History","text":""},{"location":"HISTORY/#020-2025-10-24","title":"0.2.0 (2025-10-24)","text":"<ul> <li>Added select_variable_features method to allow filtering for highly variable features based on mean-variance trend deviation.</li> </ul>"},{"location":"HISTORY/#014-2025-10-23","title":"0.1.4 (2025-10-23)","text":"<ul> <li>Fixed a minor bug in filtering functions that would sometimes cause failure when running multiple filters in succession.</li> </ul>"},{"location":"HISTORY/#013-2025-10-17","title":"0.1.3 (2025-10-17)","text":"<ul> <li>Added loads of extra documentation and usage examples.</li> </ul>"},{"location":"HISTORY/#012-2025-10-16","title":"0.1.2 (2025-10-16)","text":"<ul> <li>Patched bug in limma_trend_dea that would sometimes break design_formula.</li> </ul>"},{"location":"HISTORY/#011-2025-10-16","title":"0.1.1 (2025-10-16)","text":"<ul> <li>Patched bug that accidentally omitted R modules.</li> </ul>"},{"location":"HISTORY/#010-2025-10-14","title":"0.1.0 (2025-10-14)","text":"<ul> <li>First (beta) release of on PyPI.</li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#depy.summarized_py.SummarizedPy","title":"<code>SummarizedPy</code>","text":"<p>A class to hold bulk proteomics (and metabolomics) data and process it for differential expression analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A 2D array with shape=(features, samples) holding numerical intensity data.</p> <code>None</code> <code>features</code> <code>DataFrame</code> <p>A DataFrame holding feature metadata.</p> <code>None</code> <code>samples</code> <code>DataFrame</code> <p>A DataFrame holding sample metadata.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>ndarray</code> <p>A 2D array with shape=(features, samples) holding numerical intensity data.</p> <code>features</code> <code>DataFrame</code> <p>A DataFrame holding feature metadata.</p> <code>samples</code> <code>DataFrame</code> <p>A DataFrame holding sample metadata.</p> <code>history</code> <code>list</code> <p>A list of strings documenting each valid class module call.</p> <code>results</code> <code>DataFrame</code> <p>A DataFrame holding DEA results generated by limma_trend_dea method.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If supplied data, features, or samples are incorrect classes.</p> <code>TypeError</code> <p>If data.shape[0] != features.shape[0] or data.shape[1] != samples.shape[0].</p> <p>Examples:</p> <p>Constructing SummarizedPy object from numpy array and pandas DataFrame.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; data = np.array([[1, 2, 3],\n&gt;&gt;&gt;                 [4, 5, 6],\n&gt;&gt;&gt;                 [7, 8, 9]])\n&gt;&gt;&gt; features = pd.DataFrame({\"proteinID\": [\"feature1\", \"feature2\", \"feature3\"]})\n&gt;&gt;&gt; samples = pd.DataFrame({\"sample\": [\"sample1\", \"sample2\", \"sample3\"]})\n&gt;&gt;&gt; sp = dp.SummarizedPy(data=data, features=features, samples=samples)\n&lt;SummarizedPy(data=ndarray(shape=(3, 3), dtype=int64), features=DataFrame(shape=(3, 1)), samples=DataFrame(shape=(3, 1)))&gt;\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.filter_features","title":"<code>filter_features(expr=None, mask=None)</code>","text":"<p>Filter SummarizedPy object based on feature metadata, using either Pandas-like query strings or a mask.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>A Pandas-style query string that can be interpreted by pd.obj.query(expr=expr).</p> <code>None</code> <code>mask</code> <code>str</code> <p>A boolean mask for subsetting.</p> <code>None</code> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A filtered <code>SummarizedPy</code> object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid <code>expr</code> or <code>mask</code> argument is supplied.</p> <p>Examples:</p> <p>Filter out reverse hits in example dataset PXD000438.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; import re\n&gt;&gt;&gt; sp = dp.SummarizedPy().load_example_data()\n&gt;&gt;&gt; rev_hits = sp.features[\"protein_id\"].apply(lambda x: bool(re.match(\"REV\", x)))\n&gt;&gt;&gt; sp.features[\"rev\"] = rev_hits\n&gt;&gt;&gt; sp = sp.filter_features(expr=\"~rev\")\n&gt;&gt;&gt; sp = sp.filter_features(mask=~rev_hits) Or using rev_hits as a boolean mask\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.filter_missingness","title":"<code>filter_missingness(frac=0.75, strategy='all_conditions', condition_column=None)</code>","text":"<p>Filter SummarizedPy object based on % feature missingness across one of: overall, all conditions, or any condition.</p> <p>Parameters:</p> Name Type Description Default <code>frac</code> <code>float</code> <p>Minimum percentage valid values. Features with missingness greater than or equal to (1 - frac) will be excluded.</p> <code>0.75</code> <code>strategy</code> <code>(overall, all_conditions, any_condition)</code> <p>Filtering strategy:</p> <ul> <li>'overall' : Require &gt;= frac valid values across all samples.</li> <li>'all_conditions' : Require &gt;= frac valid values in each   condition defined by <code>condition_column</code>.</li> <li>'any_condition' : Require &gt;= frac valid values in at least   one condition defined by <code>condition_column</code>.</li> </ul> <code>'overall'</code> <code>condition_column</code> <code>str</code> <p>Name of column in the <code>samples</code> attribute on which to base filtering, in case of 'all_conditions' or 'any_condition'.</p> <code>None</code> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A filtered <code>SummarizedPy</code> object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid <code>strategy</code> is supplied or if <code>condition_column</code> is required but missing.</p> <p>Examples:</p> <p>Filter out missing values in example dataset PXD000438.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; sp = dp.SummarizedPy().load_example_data()\n&gt;&gt;&gt; sp = sp.filter_missingness(strategy=\"overall\", frac=0.75)\n&gt;&gt;&gt; sp = sp.filter_missingness(strategy=\"any_condition\", condition_column=\"condition\", frac=0.75)\n&gt;&gt;&gt; sp = sp.filter_missingness(strategy=\"all_conditions\", condition_column=\"condition\", frac=0.75)\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.filter_samples","title":"<code>filter_samples(expr=None, mask=None)</code>","text":"<p>Filter SummarizedPy object based on sample metadata, using either Pandas-like query strings or a mask.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>A Pandas-style query string that can be interpreted by pd.obj.query(expr=expr).</p> <code>None</code> <code>mask</code> <code>bool</code> <p>A boolean mask for subsetting.</p> <code>None</code> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A filtered <code>SummarizedPy</code> object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid <code>expr</code> or mask argument is supplied.</p> <p>Examples:</p> <p>Filter for ADC samples in example dataset PXD000438.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; sp = dp.SummarizedPy().load_example_data()\n&gt;&gt;&gt; sp.samples[\"condition\"] = [\"ADC\"] * 6 + [\"SCC\"] * 6\n&gt;&gt;&gt; sp = sp.filter_samples(expr=\"condition=='ADC'\")\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.import_from_delim_file","title":"<code>import_from_delim_file(path, delim, data_selector=None, feature_selector=None, replace_val_with_nan=None, clean_column_names=False)</code>  <code>classmethod</code>","text":"<p>Alternative constructor from file. Import method that reads data directly from delimited file, including feature data, feature metadata, and sample metadata, and assigns them to data, features, and samples attributes automatically. This is intended for convenient import of standardized outputs like MaxQuant's proteingroups.txt or FragPipe/DIA-NN's diann-output.pg_matrix.tsv. Method uses ColumnSelector objects to assign columns to their relevant storage containers. If no ColumnSelector is provided, the function defaults to assigning all numerical (float64) columns to data and all string (object) columns to feature data. Thus, it is best to explicitly state which columns to import. The samples attribute is automatically populated with the column names from data. The original data row and columns indices are stored in features and samples 'orig_index' variables, resp., for bookkeeping. The read path and delimiter used will be appended to the history attribute. Values in data can be replaced with NaN to indicate missingness (e.g. intensity values of 0). Column names can be automatically cleaned with the pyjanitor clean_names function.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to file to read in.</p> required <code>delim</code> <code>str</code> <p>Delimiter to parse file (e.g. '     ' for .txt or ',' for .csv).</p> required <code>data_selector</code> <code>ColumnSelector</code> <p>A <code>ColumnSelector</code> object with specified names or regex patterns to extract data columns in file. If not specified, defaults to all 'number' dtype columns.</p> <code>None</code> <code>feature_selector</code> <code>ColumnSelector</code> <p>A <code>ColumnSelector</code> object with specified names or regex patterns to extract feature metadata columns in file. If not specified, defaults to all object dtype columns.</p> <code>None</code> <code>replace_val_with_nan</code> <code>float</code> <p>A float numeric value in data to replace with np.nan to indicate missingness (e.g. 0).</p> <code>None</code> <code>clean_column_names</code> <code>bool</code> <p>Whether to clean column names in file before processing. Note that column selection happens on the cleaned column names! Thus, you have to account for this when instantiating the <code>ColumnSelector</code> object. Cleaning will coerce all string to lower case; spaces and hyphens will be replaced with underscores, and leading and trailing whitespace will be trimmed.</p> <code>False</code> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A <code>SummarizedPy</code> object.</p> <p>Examples:</p> <p>Read in data from protein groups file (e.g. MQ or FragPipe) and construct SummarizedPy object. Default to placing all numerical columns in 'data', assocaited column names in 'samples', and object or string type columns in 'features'.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; pg_path = \"~/path/to/my/proteingroups.txt\"\n&gt;&gt;&gt; sp = dp.SummarizedPy().import_from_delim_file(path=pg_path, delim=' ', replace_val_with_nan=0., clean_column_names=True)\n</code></pre> <p>Select columns to import using ColumnSelector object. Assume data are in columns containing sub-string 'LFQ_intensity_'.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; pg_path = \"~/path/to/my/proteingroups.txt\"\n&gt;&gt;&gt; data = dp.ColumnSelector(regex=\"LFQ_intensity_\")\n&gt;&gt;&gt; features = dp.ColumnSelector(names=[\"proteinID\", \"geneSymbol\", \"proteinDescription\"])\n&gt;&gt;&gt; sp = dp.SummarizedPy().import_from_delim_file(path=pg_path, delim=' ', data_selector=data, feature_selector=features)\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.impute_missing_values","title":"<code>impute_missing_values(method=None, extra_args=None)</code>","text":"<p>Impute missing values using the ImputeLCMD R package.</p> <p>Several common methods are available under the assumptions of: - MAR (KNN, SVD, MLE) - MNAR (QRILC, MinDet, MinProb) - Both MAR and MNAR (Hybrid)</p> <p>Refer to the ImputeLCMD package documentation for further information: https://cran.r-project.org/web/packages/imputeLCMD/imputeLCMD.pdf</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>(Hybrid, KNN, SVD, MLE, QRILC, MinDet, MinProb)</code> <p>Imputation method to apply: - 'Hybrid' : Uses an empirical approach (quantile regression) to find a threshold   below which missing values are imputed according to MNAR and above which values   are imputed according to MAR. Defaults to mar='KNN' and mnar='QRILC'. - 'KNN' : Uses K-nearest neighbors to impute missing values under a MAR assumption.   Defaults to k=15 neighbors. - 'SVD' : Uses Singular Value Decomposition to impute missing values under a MAR   assumption. Defaults to k=2 principal components. - 'MLE' : Uses Maximum Likelihood Estimation (EM algorithm) to impute missing values   under a MAR assumption. - 'QRILC' : Uses Quantile Regression for Imputation of Left Censored Data to impute   missing values under an MNAR assumption. Defaults to tune_sigma=1 (SD of the MNAR   distribution). - 'MinDet' : Uses imputation by minimum detected value under an MNAR assumption.   Defaults to q=0.01 (quantile for minimum value estimation). - 'MinProb' : Uses imputation by random draws from a Gaussian distribution centered on   the minimum value. Defaults to q=0.01 and tune_sigma=1.</p> <code>'Hybrid'</code> <code>extra_args</code> <code>dict</code> <p>Used in conjunction with methods that take additional parameters.</p> <p>Valid key-value pairs include:</p> <ul> <li>'mar' : {'KNN', 'SVD', 'MLE'} with <code>method='Hybrid'</code>.</li> <li>'mnar' : {'QRILC', 'MinDet', 'MinProb'} with <code>`method='Hybrid'</code>.</li> <li>'k' : int   Number of neighbors (KNN) or principal components (SVD).</li> <li>'q' : float   Quantile to estimate minimum value (MinDet, MinProb).</li> <li>'tune_sigma' : float   SD of the MNAR distribution (QRILC, MinProb).</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A <code>SummarizedPy</code> object with imputed missing values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid value is supplied to 'method'.</p> <p>Examples:</p> <p>Impute missing values using ImputeLCMD's hybrid strategy. Use example dataset PXD000438 after filtering excessive missingness.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; sp = dp.SummarizedPy().load_example_data()\n&gt;&gt;&gt; sp = sp.filter_missingness(strategy=\"overall\")\n&gt;&gt;&gt; sp = sp.impute_missing_values(method=\"Hybrid\")\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.limma_trend_dea","title":"<code>limma_trend_dea(design_formula=None, contrasts=None, feature_id_col=None, robust=False, block=None, array_weights=False, extra_args=None)</code>","text":"<p>Run differential expression analysis (DEA) with limma-trend. Limma powers its analyses by incorporating an empirical mean-variance trend estimated from the data as a prior. This alleviates the issue of estimating fold changes in the face of heteroscedasticity. In short, low-abundant features are prone to false positives due to inherently lower variance, whereas the opposite is true for high-abundant features, which are prone to false negatives. By modeling the overall mean-variance trend in the data and incorporating it as prior, information is shared across samples (which powers low-N designs) and features are effectively regularized. Compared to traditional parametric statistics, this Bayesian approach has consistently been found to be more powerful and achieve better FDR (false discovery rate) control. Additionally, a robust approximation can be used if the data contain hypo-/hypervariable features to avoid skewing the mean-variance trend. By fundamentally utilizing linear models, limma can accommodate complex designs, including fixed and random factors (i.e. mixed effects, such as nested factors or repeated measures) and their combination (i.e. to model between- and within-subjects designs). Limma can also incorporate sample quality weights, which are extremely powerful, especially in noisy datasets, which is often the case with human or animal samples. Using limma's arrayWeights function, samples are up- or down-weighted based on how variable they are compared to the average sample. Importantly, this function takes the experimental design into account when estimating the sample weights. Moreover, the user can provide an arbitrary design or none at all to estimate averaged weights for different groups of samples (i.e. in cases where sample quality is known to be especially poor according to some condition or technical covariate) or simply estimate sample-specific weights independent of design covariates. arrayWeights is run with the 'REML' method that allows for missing values. The weights are inversely proportional to sample variability (i.e. a weight of 0.5 is twice as variable as the average sample; weights &gt;1 are less variable than the average and tend to reflect higher quality). The weights can be stabilized further and squeezed towards 1 by increasing the 'prior_n' parameter &gt;10 (default); this tends to make weights more symmetric around 1 (average/equality), thus up- and down-weighting samples by similar magnitudes, rather disproportionately up-weighting good samples.</p> <p>Parameters:</p> Name Type Description Default <code>design_formula</code> <code>str</code> <p>A formula describing the linear model. Covariates must be present in samples attribute. Must begin with a tilde (~) and add covariates with '+'. Note: formula may not contain intercept term</p> <code>None</code> <code>contrasts</code> <code>dict</code> <p>A dictionary containing contrast labels (keys) and contrast definitions (values). Contrasts are defined by adding or subtracting levels of the covariates included in the design formula. Additional scaling factors are allowed, such as dividing by the number of included terms to get the average.</p> <code>None</code> <code>feature_id_col</code> <code>str</code> <p>The name of a column in the features attribute to name features by. If None, the method defaults to naming features according to their index.</p> <code>None</code> <code>robust</code> <code>bool</code> <p>Whether to run limma-trend with robust approximation.</p> <code>False</code> <code>block</code> <code>str</code> <p>Name of a column in samples attribute to using as a blocking variable. This must be used if running a model with both between- and within-subjects factors. The blocking variable should correspond to the column (subject) that gave rise to repeated values.</p> <code>None</code> <code>array_weights</code> <code>bool</code> <p>Whether to estimate sample quality weights.</p> <code>False</code> <code>extra_args</code> <code>dict</code> <p>Used in conjunction with array_weights to specify additional arguments. Valid key-value pairs include:</p> <ul> <li>prior_n : int, The number of prior features to add (defaults to 10) to increase squeezing toward 1.</li> <li>var_group : str, Name of a column in samples indicating groups (levels) that should be assigned different average weights.</li> <li>sample_id_col : str, A column in samples attribute to use for sample labelling.   This makes reading the sample weights output in limma_trend_dea.log easier.   Note that names must be unique and may not start with numbers.   If None, defaults to naming samples according to their index.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A <code>SummarizedPy</code> object with a <code>results</code> attribute containing limma-trend DEA results with columns:</p> <ul> <li>contrast_label : name of the contrast</li> <li>contrast : contrast definition</li> <li>feature : feature name</li> <li>logfc : log2 fold change (i.e. regression coefficient)</li> <li>ci_l : lower confidence interval for logfc</li> <li>ci_r : upper confidence interval for logfc</li> <li>aveexpr : average feature expression level</li> <li>t : t-value for the associated test</li> <li>p_value : nominal p-value for the associated test</li> <li>adj_p_val : Benjamini-Hochberg-based false discovery rate</li> <li>b : log-odds of differential expression</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no design_formula or contrasts arguments are provided.</p> <p>Examples:</p> <p>Full DEA pipeline on example dataset PXD000438.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; sp = dp.SummarizedPy().load_example_data()\n&gt;&gt;&gt; sp.samples[\"condition\"] = [\"ADC\"] * 6 + [\"SCC\"] * 6 # Add condition variable\n&gt;&gt;&gt; sp = sp.filter_missingness(strategy=\"overall\") # Pre-process\n&gt;&gt;&gt; sp = sp.transform_features(method=\"log\", by=2)\n&gt;&gt;&gt; sp = sp.impute_missing_values(method=\"Hybrid\")\n&gt;&gt;&gt; sp = sp.surrogate_variable_analysis(mod=\"~condition\")\n&gt;&gt;&gt; des = \"~condition+sv_1+sv_2+sv_3\" # design formula (incl. 'condition' and surrogate variables)\n&gt;&gt;&gt; contr = {\"SCCvsADC\": \"SCC-ADC\"}  # define contrast (levels must be present in covariates above)\n&gt;&gt;&gt; sp = sp.limma_trend_dea(design_formula=des, contrasts=contr, array_weights=True) # with array_weights option\n&gt;&gt;&gt; sp.results # Check newly created results attribute\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.load_example_data","title":"<code>load_example_data()</code>  <code>classmethod</code>","text":"<p>Load a real-world example proteomics dataset for demonstration purposes. The function loads dataset 'PXD000438' from the ImputeLCMD package. The data were generated from a super-SILAC experiment of human adenocacinoma (ADC) and squamous cell carcinoma (SCC) samples. The dataset contains six ADC and six SCC samples and 3,709 proteomic features with raw feature intensities and missing values. Samples 092.1-3 and 441.1-3 are ADC and 561.1-3 and 691.1-3 are SCC.</p> <p>For more information about the dataset: https://proteomecentral.proteomexchange.org/cgi/GetDataset?ID=PXD000438</p> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A <code>SummarizedPy</code> object with 12 samples and 3,709 features.</p> <p>Examples:</p> <p>Load example dataset.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; sp = dp.SummarizedPy().load_example_data()\n&lt;SummarizedPy(data=ndarray(shape=(3709, 12), dtype=float64), features=DataFrame(shape=(3709, 1)), samples=DataFrame(shape=(12, 1)))&gt;\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.select_variable_features","title":"<code>select_variable_features(top_n=None, top_percentile=None, plot=False)</code>","text":"<p>Select highly variable features (HVF) based on deviation from data mean-variance trend. Uses LOWESS to fit a smooth trend to the feature-wise mean and standard deviation values. Note: if log2 transformation has not been applied using <code>transform_features(method='log',by=2)</code> it will be applied prior to fitting the mean-variance trend. Data will be returned on the original scale.</p> <p>Parameters:</p> Name Type Description Default <code>top_n</code> <code>int</code> <p>Number of top variable features to return. Mutually exclusive with <code>top_percentile</code>.</p> <code>None</code> <code>top_percentile</code> <code>int or float</code> <p>The top Nth percentile (i.e. 100-top_percentile) of variable features to return. Mutually exclusive with <code>top_n</code>.</p> <code>None</code> <code>plot</code> <code>bool</code> <p>Whether to plot the fitted mean-variance trend and highlight HVFs.</p> <code>False</code> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A <code>SummarizedPy</code> object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid <code>top_n</code> or <code>top_percentile</code> arguments are supplied.</p> <p>Examples:</p> <p>Select top 500 most variable features in example dataset PXD000438 and plot the fitted mean-variance trend.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; sp = dp.SummarizedPy().load_example_data()\n&gt;&gt;&gt; sp = sp.select_variable_features(top_n=500, plot=True)\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.surrogate_variable_analysis","title":"<code>surrogate_variable_analysis(mod=None, mod0='~1', num_sv=None)</code>","text":"<p>Run surrogate variable analysis to estimate latent factors that capture expression heterogeneity or hidden batch effects. The surrogate variables (SVs) will be added to the samples attribute and can be included as covariates in DEA. SVs are estimated through PCA on the residualized feature matrix after regressing out known experimental and technical/batch covariates. This is done by supplying the method with a fully parameterized model (mod), including all known covariates (experimental and technical; i.e. as present in the samples attribute), and a null model, including only technical (adjustment) covariates (mod0). The number of significant surrogate variables to estimate can then be specified using the num_sv argument; alternatively, the method can be run without specifying a number and allowing SVA to estimate the number empirically (using SVA's 'num.sv' function and the default 'leek' method). Note that this can return 0 SVs and fail. However, it is still possible to find significant SVs by forcing the method to run with a pre-specified num_sv argument. The mod and mod0 arguments must be specified using R formula formatting, which all start with a tilde (~) symbol and add covariates (+) and their interactions (*). Covariates must be present in the samples attribute. If no technical covariates are known, the method will run with the recommended default of \"~1\" (i.e. only using an intercept term). For more information, see: https://bioconductor.org/packages/3.19/bioc/vignettes/sva/inst/doc/sva.pdf</p> <p>Parameters:</p> Name Type Description Default <code>mod</code> <code>str</code> <p>A formula describing the fully parameterized model (incl. all known covariates). Must begin with a tilde (~) and add covariates with '+'.</p> <code>None</code> <code>mod0</code> <code>str</code> <p>A formula describing the null model (incl. all known adjustment covariates). Must begin with a tilde (~) and add covariates with '+'. Defaults to '~1'.</p> <code>'~1'</code> <code>num_sv</code> <code>int</code> <p>The number of significant surrogate variables to estimate.</p> <code>None</code> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A <code>SummarizedPy</code> object with estimate surrogate variables in the samples attribute.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no mod formula is supplied.</p> <p>Examples:</p> <p>Use SVA to estimate surrogate variables for inclusion in DEA. Use example dataset PXD000438: filter missing values and log2 transform features first.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; sp = dp.SummarizedPy().load_example_data()\n&gt;&gt;&gt; sp = sp.filter_missingness(strategy=\"overall\") # Filter excessive missinginess (this is important)\n&gt;&gt;&gt; sp = sp.transform_features(method=\"log\", by=2) # Log transform data (important)\n&gt;&gt;&gt; sp = sp.impute_missing_values(method=\"Hybrid\") # Optionally, impute missing remaining values (sva excludes any feature with nan values)\n&gt;&gt;&gt; sp = sp.surrogate_variable_analysis(mod=\"~condition\") # Default null model: mod0 = '~1' (intercept-only)\n&gt;&gt;&gt; sp.samples  # SVs now in samples attribute\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.transform_features","title":"<code>transform_features(method, axis=None, by=None)</code>","text":"<pre><code>transform_features(\n    method: Literal[\"center\"],\n    axis: int,\n    by: Optional[Literal[\"mean\", \"median\"]] = None,\n) -&gt; SummarizedPy\n</code></pre><pre><code>transform_features(\n    method: Literal[\"log\"], by: Optional[int] = None\n) -&gt; SummarizedPy\n</code></pre><pre><code>transform_features(\n    method: Literal[\"z-score\"], axis: int, by: None = None\n) -&gt; SummarizedPy\n</code></pre><pre><code>transform_features(method: Literal['vsn']) -&gt; SummarizedPy\n</code></pre> <p>Mathematically transform features stored in data attribute using one of: log (base N), center (using mean or median subtraction), or z-score (standardize).</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>An axis to perform the transformation along:</p> <ul> <li>0 = rows (per-feature)</li> <li>1 = columns (per-sample).</li> </ul> <code>None</code> <code>method</code> <code>(log, center, z - score, vsn)</code> <p>Mathematical transformation to apply:</p> <ul> <li>'log' : Applies log base N (use <code>by</code> parameter to set base) transformation across entire data array.</li> <li>'center' : Center data by subtraction (use in conjunction with <code>by</code> parameter).</li> <li>'z-score' : Standardizes data using z-score transformation (i.e. (x_i-x_mean)/x_std). NB: applies Bessel's N-1 correction to estimate sample standard deviation.</li> <li>vsn : Applies variance stabilizing normalization, as implemented Huber et al. (2004).</li> </ul> <code>'log'</code> <code>by</code> <code>str or int</code> <p>Used in conjunction with <code>method='center'</code> or <code>method='log'</code>.</p> <ul> <li>With 'center' : str, One of 'mean' or 'median' to subtract axis mean or median from each cell.</li> <li>With 'log' : int, An integer for the base of the logarithm (defaults to 2).</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>SummarizedPy</code> <p>A transformed SummarizedPy object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid value is supplied to either axis, method, or by.</p> <p>Examples:</p> <p>Transform feature data in example dataset PXD000438.</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; sp = dp.SummarizedPy().load_example_data()\n&gt;&gt;&gt; sp = sp.transform_features(method=\"log\", by=2) # Log transformation (base 2)\n&gt;&gt;&gt; sp = sp.transform_features(method=\"center\", by=\"median\", axis=1) # Center data sample-wise by median\n&gt;&gt;&gt; sp = sp.transform_features(method=\"z-score\", axis=0) # Feature-wise standardization\n&gt;&gt;&gt; sp = sp.transform_features(method=\"vsn\") # vsn normalization\n</code></pre>"},{"location":"api/#depy.summarized_py.SummarizedPy.volcano_plot","title":"<code>volcano_plot(contrasts=None, top_n=3, de_colors=None)</code>","text":"<p>Generate volcano plots for limma-trend results, highlighting the top N up- and downregulated features.</p> <p>Parameters:</p> Name Type Description Default <code>contrasts</code> <code>list</code> <p>A list of strings referring to the name of contrasts to plot (i.e. column 'contrast_label' in results attribute). Defaults to all contrasts.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>Number of top up- and down-regulated features to highlight, ranked by adjusted p-value (FDR). Note: top_n up- and top_n downregulated features will be displayed, rather top_n in total. Defaults to top 3.</p> <code>3</code> <code>de_colors</code> <code>dict</code> <p>Colors to use for upregulated, downregulated, and non-significant features. Must supply 'Up', 'Down', and 'ns' as keys with associated colors (str).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionaries of matplotlib figure and axes objects for each contrast. - fig : <code>.Figure</code> - ax : <code>~matplotlib.axes.Axes</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid value is supplied to either contrasts or de_colors.</p> <p>Examples:</p> <p>Generate volcano plots after running limma_trend_dea method.</p> <pre><code>&gt;&gt;&gt; sp.volcano_plot()\n&gt;&gt;&gt; sp.volcano_plot(contrasts=[\"SCCvsADC\"])\n</code></pre>"},{"location":"api/#depy.column_selector.ColumnSelector","title":"<code>ColumnSelector</code>","text":"<p>Object for pre-selecting columns when constructing SummarizedPy from file.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>list</code> <p>A list of strings matching column names.</p> <code>None</code> <code>regex</code> <code>str</code> <p>A string that can be interpreted as a regular expression by re.search. Note that the search is case-insensitive.</p> <code>None</code> <p>Examples:</p> <p>Define columns to import from file. Assume data are in columns labeled \"LFQ_intensity_*\".</p> <pre><code>&gt;&gt;&gt; import depy as dp\n&gt;&gt;&gt; data = dp.ColumnSelector(regex=\"LFQ_intensity_\")\n&gt;&gt;&gt; features = dp.ColumnSelector(names=[\"proteinID\", \"geneSymbol\", \"proteinDescription\"])\n&gt;&gt;&gt; sp = dp.SummarizedPy().import_from_delim_file(path=\"my/path/proteingroups.txt\", delim=' ', data_selector=data, feature_selector=features)\n</code></pre>"},{"location":"api/#depy.column_selector.ColumnSelector.select_cols","title":"<code>select_cols(df)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Pandas <code>DataFrame</code> object on which to perform column selection.</p> required <p>Returns:</p> Type Description <code>    DataFrame</code> <p>A pandas <code>DataFrame</code> with selected columns.</p> <p>Raises:</p> Type Description <code>    KeyError</code> <p>If no valid column names are supplied.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install DEPy and all its dependencies, please clone the DEPy conda environment:</p> <p><pre><code>conda env create -f environment.yml\nconda activate depy\n</code></pre> This is the intended method of installation. You can install  the 'summarizedpy' package on its own using pip, but please don't.</p>"},{"location":"installation/#pip","title":"pip","text":"<p><pre><code>pip install summarizedpy\n</code></pre> Note that the SummarizedPy package must be run within the 'depy' conda environment or one cloned from it.</p> <p>In short, this is because summarizedpy runs R as a subprocess in a highly defined environment to isolate it from the user's global R environment. This is necessary to ensure that Bioconductor packages load properly.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#getting-started","title":"Getting started","text":"<p>To use DEPy in a project, start the 'depy' conda environment:</p> <p><pre><code>conda activate depy\n</code></pre> Then, open a script or a Jupyter Notebook and simply: <pre><code>import depy as dp\n</code></pre></p>"},{"location":"usage/#example-workflow","title":"Example workflow","text":""},{"location":"usage/#loading-the-data","title":"Loading the data","text":"<p>Let's load the example dataset that comes with DEPy (courtesy of the ImputeLCMD package). This is a real-world proteomics dataset of human cancer cell lines (3,709 features, 12 samples). Data were processed with MaxQaunt and comes in the form of protein groups and their intensities. <pre><code>import depy as dp\n\nsp = dp.SummarizedPy()\nsp = sp.load_example_data()\n</code></pre></p>"},{"location":"usage/#exploring-the-summarizedpy-object","title":"Exploring the SummarizedPy object","text":"<p>Data are stored in three main attributes: - data (numpy ndarray with float or int dtype) - features (pandas DataFrame) - samples (pandas DataFrame)</p> <p>These can be readily accessed <pre><code># Check expression data\nsp.data\n\n# Check feature metadata\nsp.features\n\n# Check sample metadata\nsp.samples\n</code></pre> To check current dimensions, we can simply invoke the object or call <code>print()</code> on it <pre><code># See current dimensions in 'repr' format\nsp\n\n# Get a user-friendly summary of the entire object\nprint(sp)\n</code></pre> The last statement will reveal another useful attribute, the <code>history</code> attribute. <pre><code># Check history attribute\nsp.history\n</code></pre> This attribute keeps a faithful record of everything you do to the <code>SummarizedPy</code> object, including function calls and parameters. This is incredibly handy for reproducibility.</p>"},{"location":"usage/#subsetting-and-slicing","title":"Subsetting and slicing","text":"<p><code>SummarizedPy</code> objects can be subset and sliced just like SummarizedExperiment in R. The objects are indexed as <code>sp[features, samples]</code> Thus, we can:</p> <p><pre><code># Get first feature and all samples\nsp[1, :]\n\n# Or equivalently\nsp[1]\n\n# Get first sample and all features\nsp[:, 1]\n</code></pre> Note that if you subset your <code>SummarizedPy</code>, it will be reflected in the <code>history</code> attribute:</p> <pre><code># Subset first feature and all samples\nsp = sp[1]\n\n# Check history\nsp.history\n</code></pre>"},{"location":"usage/#a-note-on-dimensionality","title":"A note on dimensionality","text":"<p><code>SummarizedPy</code> enforces a 2D constraint on all three main attributes <code>data features samples</code> such that you always get a 2D <code>numpy</code> array when calling <code>sp.data</code> and a full <code>pandas</code> DataFrame when calling <code>sp.features</code> or <code>sp.samples</code></p> <p>Critically, <code>SummarizedPy</code> enforces the following rules:</p> <p><pre><code>sp.data.shape[0] == sp.features.shape[0]\nsp.data.shape[1] == sp.samples.shape[0]\n</code></pre> Indeed, if you were to try <pre><code>import numpy as np\nimport pandas as pd\n\ndata = np.array([[1, 2, 3],\n                 [4, 5, 6]])\nfeatures = pd.DataFrame({\"feature_id\": [\"feature1\", \"feature2\", \"feature3\"]})\nsamples = pd.DataFrame({\"sample_id\": [\"sample1\", \"sample2\"]})\n\nsp = dp.SummarizedPy(data=data,\n                features=features,\n                samples=samples)\n</code></pre> You would get a <code>ValueError</code> saying <code>Number of samples (2) does not match number of columns in data (3)</code></p> <p>This is because <code>SummarizedPy</code> maps <code>samples</code> and <code>features</code> to <code>data</code>by indexing. Thus, order of rows in these attributes is the source of truth.</p> <p>As a consequence, re-assigning <code>data</code> is not possible and will raise an <code>AttributeError</code></p> <pre><code>import numpy as np\nimport pandas as pd\n\ndata = np.array([[1, 2, 3],\n                 [4, 5, 6]])\nfeatures = pd.DataFrame({\"feature_id\": [\"feature1\", \"feature2\", \"feature3\"]})\nsamples = pd.DataFrame({\"sample_id\": [\"sample1\", \"sample2\", \"sample3\"]})\n\nsp = dp.SummarizedPy(data=data,\n                features=features,\n                samples=samples)\n\n# Trying to re-assign .data will raise AttributeError\nsp.data = data\n</code></pre> <p>Similarly, you will not be able to re-assign <code>.history</code> or <code>.results</code> (we will see this one later)</p> <p>You can however mutate in-place, but this will not be reflected in <code>history</code> and should therefore be done at your own peril. We like audit trails, right? <pre><code># Mutate in-place possible but not recommended\nsp.data[1,1] = 10\n</code></pre></p>"},{"location":"usage/#filtering-by-sample-and-feature-metadata","title":"Filtering by sample and feature metadata","text":"<p>We can filter the entire <code>SummarizedPy</code> object based on variables in the <code>.features</code> and <code>.samples</code> attributes. This is easily done using the <code>filter_features</code> and <code>filter_samples</code> methods, resp. These functions take a <code>pandas query()</code> style expression or a boolean mask. Returning to our <code>load_example_data</code> example, we can filter based on sample, metadata:</p> <p><pre><code># Filtering samples\n# Include sample condition variable (first six = ADC; last six = SCC)\nsp.samples[\"condition\"] = [\"ADC\"] * 6 + [\"SCC\"] * 6\n\n# Filter for SCC samples using pandas query expression\nsp = sp.filter_samples(expr=\"condition=='SCC'\")\n\n# Check dimensions\nsp\n\n# Check history\nsp.history\n</code></pre> Similarly, we can filter based on feature metadata:</p> <pre><code># Filtering features\n# Create boolean vector indicating reverse hits\nimport re\nrev_hits = sp.features[\"protein_id\"].apply(lambda x: bool(re.match(\"REV\", x)))\n\n# Add as feature metadata\nsp.features[\"rev\"] = rev_hits\n\n# Filter out reverse hits using pandas query expression\nsp = sp.filter_features(expr=\"~rev\")\n\n# Alternatively, use a boolean mask\nsp.filter_features(mask=~rev_hits)\n\n# Check dimensions\nsp\n\n# Check history\nsp.history\n</code></pre>"},{"location":"usage/#missingness-filtering","title":"Missingness filtering","text":"<p>In MS-based -omics, missing data is a guarantee. Whether you have DDA or DIA, there will NA (or nan) values. When conducting differential expression analyses, you need sufficient valid values to base your fold changes on. It makes no sense trying to compute a fold change if one group has 90% missing values for some feature (at least not with linear models). We will not go into all the reasons for missingness, when it is and when it is not a problem; rather, we will simply assume that we want some ceiling on missingness.</p> <p>In Perseus (and proteomics more generally), the standard approaches to missingness filtering include: - % valid values across all samples - % valid values in at least one experimental condition - % valid values in every experimental condition</p> <p>DEPy caters to all three and lets you set the threshold, expressed as the fraction of valid (i.e. non-nan) values. Simply set the <code>strategy=</code> to one of - <code>overall</code> (across all samples) - <code>any_condition</code> (at least one condition) - <code>all_conditions</code> (each condition)</p> <p>Note that if you use one of the condition-based methods, you need to set the <code>condition_column</code> parameter to indicate the name of the column in <code>samples</code> to filter on.</p> <p>Finally, set the <code>frac</code> parameter to the fraction of minimum valid values required (default to 0.75; i.e. $\\ge$75% valid value).</p> <pre><code># Confirm the presence of nan values\nimport numpy as np\nnp.isnan(sp.data).any()\n\n# Missingness filtering method\n# Across all samples (i.e. independent of condition)\nsp = sp.filter_missingness(strategy=\"overall\", frac=0.75)\n\n# At least one condition (i.e. as a fraction of either SCC or ADC)\nsp = sp.filter_missingness(strategy=\"any_condition\",\n                      condition_column=\"condition\",\n                      frac=0.75)\n\n# In each condition (i.e. as a fraction of both SCC and ADC)\nsp = sp.filter_missingness(strategy=\"all_conditions\",\n                      condition_column=\"condition\",\n                      frac=0.75)\n\n# Check dimensions\nsp\n\n# Check history\nsp.history\n</code></pre>"},{"location":"usage/#highly-variable-feature-selection","title":"Highly variable feature selection","text":"<p>Sometimes, you have too many features to analyze. This can reduce your power in DEA due to the multiple comparisons problem. In machine learning, you may want to start building a regularized model with a smaller subset of features so as not to waste computational time on low-variance features.</p> <p>DEPy has a built-in solution for this with its <code>select_variable_features</code> method. When selecting high-variance features, it is important to account for the heteroscedasticity in the data. Otherwise, we would end up biasing our selection, as the underlying feature variance increases as a function of the average intensity.</p> <p>Similar to the approach taken by <code>Seurat</code>, DEPy models the mean-variance trend of the data by fitting a LOWESS model to the feature-wise means and standard deviations. Note that this calculation will be done on log transformed data. DEPy detects whether a log transformation has been done previously based on the object's <code>history</code> attribute and runs one if not (the data will be returned un-transformed). It then computes standardized residuals (i.e. the deviation from the fitted dispersion values) and ranks the features based on these z-scores.</p> <p>You can then choose to return either the <code>top_n</code> (e.g. top 100) or <code>top_percentile</code> (e.g. top 5th percentile) of variable features. Additionally, the method can display a plot of the data's mean-variance relationship, the fitted LOWESS trend, and highly variable features (HVF) labeled.</p> <pre><code># Return the top 100 most variable features and show mean-variance trend plot\nsp = sp.select_variable_features(top_n=100, plot=True)\n\n# Return the top 5% most variable features (calculated as 100-top_percentile)\nsp = sp.select_variable_features(top_percentile=5, plot=True)\n\n# Check history\nsp.history\n</code></pre>"},{"location":"usage/#transformations","title":"Transformations","text":"<p>There are several common data transformations and normalizations procedures in proteomics and metabolomics, each of which can be performed in a sample- or feature-wise manner. - log transformation (partly addresses the inherent heteroscedasticity in intensity data) - centering (subtracting a constant such as mean or median to remove offset differences) - standardization (aka z-scoring; enforces unit variance and symmetry about the mean, i.e. 0)</p> <p>Another powerful transformation that addresses both heteroscedasticity and can reduce intra-group variance is the variance stabilizing normalization, popularized by the R package <code>vsn</code>. In short, it uses a generalized logarithm with a linear transformation to remove the asymptote at 0 that otherwise occurs with the standard log transform. The affine (linear) transformation performs a column-wise centering and scaling with parameters estimated empirically. This method, initially introduced for microarray studies, has its roots in error models developed for gas chromatography. Check out the original paper by Wolfgang Huber et al. I personally use this a lot for untargeted metabolomics and with other tools such as MOFA.</p> <p>While many more transformations exist, the three listed ones are arguably the most common in proteomics. DEPy gives you the ability to perform each one in a column- or row-wise manner using the <code>transform_features</code> method. It uses numpy to make them blazingly fast where possible, and calls R for vsn.</p> <p>The three main parameters are: - method: one of <code>'log'</code>, <code>'center'</code>, <code>'z-score'</code>, <code>'vsn'</code> - by: one of <code>'mean'</code> or <code>'median'</code> for <code>method='center'</code> or an <code>int</code> for <code>method='log'</code> - axis: <code>0</code> for row-wise (feature-wise),  <code>1</code> column-wise (sample-wise) (omitted for methods <code>log</code> and <code>vsn</code>)</p> <pre><code># Log transformation (base 2)\nsp = sp.transform_features(method=\"log\", by=2)\n\n# Center data sample-wise by median\nsp = sp.transform_features(method=\"center\", by=\"median\", axis=1)\n\n# Feature-wise standardization\nsp = sp.transform_features(method=\"z-score\", axis=0)\n\n# vsn normalization\nsp = sp.transform_features(method=\"vsn\")\n\n# Check data\nsp.data\n\n# Check history\nsp.history\n</code></pre>"},{"location":"usage/#imputation","title":"Imputation","text":"<p>Depending on whom you ask, this is either a valid or questionable idea. We will not debate it here. I would simply note (based on having conducted systematic trials) that you should only impute provided you have a sufficient number of valid values to base the imputation on. What constitutes sufficient is like asking 'how long is a piece of string', but if I had to pick a number, I would say $\\ge$50% valid values per condition at a minimum.</p> <p>For our purposes here, missingness comes in two flavors: - MAR (missing at random): the missingness pattern is randomly distributed and independent of the features - MNAR (missing not at random): usually left-censored data in MS-based -omics, whereby features are missing due to low abundance.</p> <p>DEPy runs the ImputeLCMD R package under the hood. This is a great package, created by Cosmin Lazar et al. It comes with methods for MAR, MNAR, and critically, hybrid MAR-MNAR assumptions. Which assumption to use is beyond the scope of this tutorial, but check out Lazar's excellent paper on the topic.</p> <p>Accordingly, DEPy comes with all of those methods and supports all native parameters. Exploring them all is a bit much, so we will only demonstrate the <code>hybrid</code> method.</p> <pre><code># Filter excessive missinginess\nsp = sp.filter_missingness(strategy=\"overall\")\n\n# Impute remaining missing values with hybrid approach\n# MAR = KNN (default)\n# MNAR = QRILC (default)\nsp = sp.impute_missing_values(method=\"Hybrid\", extra_args={\"mar\": \"KNN\", \"mnar\": \"QRILC\"})\n\n# Check history\nsp.history\n</code></pre>"},{"location":"usage/#surrogate-variable-analysis","title":"Surrogate variable analysis","text":"<p>This is an excellent tool for any bioinformatician to have in their toolbox. Indeed, it has proven invaluable in some of my own work, which deals with real-world, messy data from humans and animals. The method is surprisingly intuitive for all its seeming complexity. Check out the original paper by Leek et al.; it is a fantastic read.</p>"},{"location":"usage/#why-use-sva","title":"Why use sva","text":"<p>Sometimes, we have known batch effects or technical covariates that can be readily included in our models as adjustment variables. However, sometimes, there can be hidden batch effects in your data that manifest as latent noise and correlations among features that we cannot account for. How do you include something you cannot measure?</p>"},{"location":"usage/#this-is-why-sva","title":"This is why sva","text":"<p>In a nutshell, surrogate variable analysis (sva) performs PCA on the residuals of the data matrix after regressing out a fully parametrized models that includes all known experimental and technical covariates. Then, it uses the resultant factors to capture latent covariation among features, check which features associates with the factors, computes a new model on the reduced set, and returns the latent factors (aka \"surrogate variables\"), which can be included as technical covariates in your subsequent model.</p> <p>neat...</p> <p>DEPy runs the <code>sva</code> R package with all its native arguments. Simply specify your fully parametrized model, including all variables of interest (both experimental ones you care about, and adjustment/technical/batch variables). Additionally, <code>sva</code> requires a so-called null model that includes only known adjustment variables. If you do not have any known ones, the default is to simply use an intercept model. Note that you must specify the models with R-style tilde expressions; e.g. <code>~var1+var2</code></p> <pre><code># Filter excessive missinginess (this is important)\nsp = sp.filter_missingness(strategy=\"overall\")\n\n# Log transform data (important)\nsp = sp.transform_features(method=\"log\", by=2)\n\n# Optionally, impute missing remaining values (sva excludes any feature with nan values)\nsp = sp.impute_missing_values(method=\"Hybrid\")\n\n# Run sva\n# Default null model: mod0 = '~1' (intercept-only)\nsp = sp.surrogate_variable_analysis(mod=\"~condition\")\n\n# Surrogate variables are added to samples attribute\nsp.samples\n\n# Check history\nsp.history\n</code></pre>"},{"location":"usage/#limma-trend","title":"limma-trend","text":"<p>Now for the actual differential expression analysis, we will use limma, brain child of Gordon Smyth (check out the legendary paper). Limma leverages an empirically estimated mean-variance trend as a prior to adjust for the heteroscedasticity common to so many -omics modalities. This Bayesian prior serves as a form shrinkage, which mitigates the inflated false positive and negative rates that come with conducting fold change analysis on low- and high-abundant features if the mean-variance is not accounted for. This is particularly useful for small sample sizes. Limma-trend was implemented and adapted in the proteomics R packages <code>DEqMS</code> and <code>DEP</code> for these very reasons. Limma has been found to be more powerful and exert better FDR control than standard parametric statistics (i.e. t-test and ANOVA). Critically, <code>limma-trend</code> performs about as well when adjusting for the mean-variance trend using the feature-wise intensity distribution as when using peptide or PSM counts to adjust protein-level variance (see DEqMS paper).</p> <p>Another massive benefit to limma is that it can incorporate sample quality weights calculated by its <code>arrayWeights</code> function. In my years, I have found that it makes all the difference when working with data from human and animal samples. In short, the weights are calculated by allowing each feature to have a sample-specific source of variation. Using the overall mean-variance trend of the dataset, each sample can be quantified in terms of how much it deviates from the average (i.e. how 'noisy' a sample is). This information is then incorporated into the weights, which are reciprocal logarithms of that deviation (e.g. a quality weight of 0.5 is 2x as variable as the average sample; i.e. likely low quality). The weights take the experimental design into account and can be estimated for each sample or averaged as a function of some covariate (e.g. if quality was found to be reliably lower due to variable).</p> <p>Finally, limma is highly flexible due to being a linear model. it can incorporate mixed effects, repeated measures, and between- and within-subjects factors.</p> <p>Limma is available with most of its native parameters. Simply specify the design, the contrasts, whether to include array weights, etc. - <code>design_formula</code> (R-style tilde expression with covariates of interest) - <code>contrasts</code> (dictionary with the names and definitions of contrasts) - <code>array_weights</code> (boolean; whether to include sample quality weights)</p>"},{"location":"usage/#a-brief-note-on-design-formulae","title":"A brief note on design formulae","text":"<p>DEPy will automatically enforce a marginal means model; that is, an intercept will not be included even if you specify one. This is to keep the design matrix tidy to ensure proper matching between its column names and contrast terms. To this end, DEPy will: - inject a '~' if you forget one - add a '0+' to the start of the <code>design_formula</code></p> <p>In general, the intercept term is often of little interest (it represents the grand average) and will not affect fold changes.</p> <pre><code># Specify design formula (including 'condition' and surrogate variables)\ndes = \"~condition+sv_1+sv_2+sv_3\"\n\n# Define contrast (levels must be present in covariates above)\ncontr = {\"SCCvsADC\": \"SCC-ADC\"}\n\n# Run limma-trend with array_weights option\nsp = sp.limma_trend_dea(design_formula=des, contrasts=contr, array_weights=True)\n\n# Check newly created results attribute\nsp.results\n\n# Check history\nsp.history\n</code></pre>"},{"location":"usage/#volcano-plots","title":"Volcano plots","text":"<p>Finally, we will plot the estimate log fold changes against the log-transformed nominal p-values per feature for each contrast. Better known as a volcano plot. This is very simple in DEPy: you can either provide a list contrast names or let DEPy produce one volcano plot per contrast. Plots are generated with <code>matplotlib</code> and returned as a tuple of dictionaries with keys = contrast names and values = <code>matplotlib.Figure</code> or <code>matplotlib.Axes</code> objects.</p> <p><pre><code># Generate volcano plots for all contrasts\nfig, ax = sp.volcano_plot()\n\n# Optionally, specify the name of a contrast\nsp.volcano_plot(contrasts=[\"SCCvsADC\"])\n</code></pre> By default, the function plots the top 3 up- and top 3 down-regulated features according to FDR. This can be changed with the <code>top_n</code> parameter. You can also change color scheme by providing a dictionary with names <code>'Up'</code>, <code>'Down'</code>, and <code>'ns'</code>, and hexcodes or valid color names as values.</p> <pre><code># Highlight top up- and down-regulated features\nfig, ax = sp.volcano_plot(top_n=1)\n\n# Change colors\nde_colors = {\"Up\": \"red\", \"Down\": \"blue\", \"ns\": \"white\"}\nsp.volcano_plot(de_colors=de_colors)\n</code></pre> <p>There you have it!</p>"}]}