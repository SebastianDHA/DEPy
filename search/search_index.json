{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to DEPy's documentation! \u00b6 DEPy (aka SummarizedPy ) is a Python library for bulk proteomics (and metabolomics) differential expression analysis. It was inspired by the R packages DEP and SummarizedExperiment. While Python containers for single-cell data exist within the scverse , tools for bulk -omics are less common, especially when it comes to proteomics. Contents \u00b6 Installation Usage API History The power of transcriptomics unleashed in MS-based -omics \u00b6 Discovery proteomics shares a lot of statistical similarities with transcriptomics data once you have the final protein- or peptide-level intensity values (like a protein groups matrix from FragPipe or MaxQuant). More importantly, bulk -omics often face similar issues: variable sample quality, hidden batch effects, heteroscedasticity, and complex designs. Fortunately, several tools have been developed in the transcriptomics literature to address these very concerns! - limma - arrayWeights - vsn - sva Additionally, there are tools to address MS-based -omics issues, such as missing value imputation (e.g. ImputeLCMD). User-friendly software like Perseus come with easy filtering, transformation, imputation, and testing. Alas, none of this is available in Python... Why DEPy \u00b6 I spent my PhD leveraging R-based transcriptomics tools to power my proteomics and metabolomics analyses and tackle common issues associated with real-world, human and animal datasets. As I was migrating my workflows to Python (mainly for ML), I wanted to bring all of that Bioconductor goodness with me. So, I decided to package it - bringing the best of two worlds. Features \u00b6 Leverage R packages like limma, arrayWeights, vsn, sva, ImputeLCMD Example proteomics dataset for testing and exploring SummarizedPy: A container for your data and metadata Numpy array-based storage for faster operations and integration with the greater Python stack I/O using Feather for memory efficiency and speed Isolated R environment run as a subprocess Easily scales to hundreds of samples and thousands of features History attribute logs every step and parameter setting High test coverage If that sounds interesting, please read on!","title":"Home"},{"location":"#welcome-to-depys-documentation","text":"DEPy (aka SummarizedPy ) is a Python library for bulk proteomics (and metabolomics) differential expression analysis. It was inspired by the R packages DEP and SummarizedExperiment. While Python containers for single-cell data exist within the scverse , tools for bulk -omics are less common, especially when it comes to proteomics.","title":"Welcome to DEPy's documentation!"},{"location":"#contents","text":"Installation Usage API History","title":"Contents"},{"location":"#the-power-of-transcriptomics-unleashed-in-ms-based-omics","text":"Discovery proteomics shares a lot of statistical similarities with transcriptomics data once you have the final protein- or peptide-level intensity values (like a protein groups matrix from FragPipe or MaxQuant). More importantly, bulk -omics often face similar issues: variable sample quality, hidden batch effects, heteroscedasticity, and complex designs. Fortunately, several tools have been developed in the transcriptomics literature to address these very concerns! - limma - arrayWeights - vsn - sva Additionally, there are tools to address MS-based -omics issues, such as missing value imputation (e.g. ImputeLCMD). User-friendly software like Perseus come with easy filtering, transformation, imputation, and testing. Alas, none of this is available in Python...","title":"The power of transcriptomics unleashed in MS-based -omics"},{"location":"#why-depy","text":"I spent my PhD leveraging R-based transcriptomics tools to power my proteomics and metabolomics analyses and tackle common issues associated with real-world, human and animal datasets. As I was migrating my workflows to Python (mainly for ML), I wanted to bring all of that Bioconductor goodness with me. So, I decided to package it - bringing the best of two worlds.","title":"Why DEPy"},{"location":"#features","text":"Leverage R packages like limma, arrayWeights, vsn, sva, ImputeLCMD Example proteomics dataset for testing and exploring SummarizedPy: A container for your data and metadata Numpy array-based storage for faster operations and integration with the greater Python stack I/O using Feather for memory efficiency and speed Isolated R environment run as a subprocess Easily scales to hundreds of samples and thousands of features History attribute logs every step and parameter setting High test coverage If that sounds interesting, please read on!","title":"Features"},{"location":"HISTORY/","text":"History \u00b6 0.5.0 (2026-02-03) \u00b6 Removed dependency on pyjanitor and implemented custom column name cleaning. 0.4.2 (2026-02-03) \u00b6 Patched a minor bug in limma_trend_dea. 0.4.1 (2025-12-04) \u00b6 Patched a minor bug in filter_features and filter_samples. 0.4.0 (2025-11-27) \u00b6 Added new features to allow saving and loading SP objects using pickle. 0.3.1 (2025-11-26) \u00b6 Added plot_pca method to visualize data with PCA. Updated package Python dependency to 3.11+ Fixed a bug due to recent dependency breaks (pyjanitor, pandas_flavor) 0.2.0 (2025-10-24) \u00b6 Added select_variable_features method to allow filtering for highly variable features based on mean-variance trend deviation. 0.1.4 (2025-10-23) \u00b6 Fixed a minor bug in filtering functions that would sometimes cause failure when running multiple filters in succession. 0.1.3 (2025-10-17) \u00b6 Added loads of extra documentation and usage examples. 0.1.2 (2025-10-16) \u00b6 Patched bug in limma_trend_dea that would sometimes break design_formula. 0.1.1 (2025-10-16) \u00b6 Patched bug that accidentally omitted R modules. 0.1.0 (2025-10-14) \u00b6 First (beta) release of on PyPI.","title":"Changelog"},{"location":"HISTORY/#history","text":"","title":"History"},{"location":"HISTORY/#050-2026-02-03","text":"Removed dependency on pyjanitor and implemented custom column name cleaning.","title":"0.5.0 (2026-02-03)"},{"location":"HISTORY/#042-2026-02-03","text":"Patched a minor bug in limma_trend_dea.","title":"0.4.2 (2026-02-03)"},{"location":"HISTORY/#041-2025-12-04","text":"Patched a minor bug in filter_features and filter_samples.","title":"0.4.1 (2025-12-04)"},{"location":"HISTORY/#040-2025-11-27","text":"Added new features to allow saving and loading SP objects using pickle.","title":"0.4.0 (2025-11-27)"},{"location":"HISTORY/#031-2025-11-26","text":"Added plot_pca method to visualize data with PCA. Updated package Python dependency to 3.11+ Fixed a bug due to recent dependency breaks (pyjanitor, pandas_flavor)","title":"0.3.1 (2025-11-26)"},{"location":"HISTORY/#020-2025-10-24","text":"Added select_variable_features method to allow filtering for highly variable features based on mean-variance trend deviation.","title":"0.2.0 (2025-10-24)"},{"location":"HISTORY/#014-2025-10-23","text":"Fixed a minor bug in filtering functions that would sometimes cause failure when running multiple filters in succession.","title":"0.1.4 (2025-10-23)"},{"location":"HISTORY/#013-2025-10-17","text":"Added loads of extra documentation and usage examples.","title":"0.1.3 (2025-10-17)"},{"location":"HISTORY/#012-2025-10-16","text":"Patched bug in limma_trend_dea that would sometimes break design_formula.","title":"0.1.2 (2025-10-16)"},{"location":"HISTORY/#011-2025-10-16","text":"Patched bug that accidentally omitted R modules.","title":"0.1.1 (2025-10-16)"},{"location":"HISTORY/#010-2025-10-14","text":"First (beta) release of on PyPI.","title":"0.1.0 (2025-10-14)"},{"location":"api/","text":"API Reference \u00b6 SummarizedPy \u00b6 A class to hold bulk proteomics (and metabolomics) data and process it for differential expression analysis. Parameters: data ( ndarray , default: None ) \u2013 A 2D array with shape=(features, samples) holding numerical intensity data. features ( DataFrame , default: None ) \u2013 A DataFrame holding feature metadata. samples ( DataFrame , default: None ) \u2013 A DataFrame holding sample metadata. Attributes: data ( ndarray ) \u2013 A 2D array with shape=(features, samples) holding numerical intensity data. features ( DataFrame ) \u2013 A DataFrame holding feature metadata. samples ( DataFrame ) \u2013 A DataFrame holding sample metadata. history ( list ) \u2013 A list of strings documenting each valid class module call. results ( DataFrame ) \u2013 A DataFrame holding DEA results generated by limma_trend_dea method. Raises: ValueError \u2013 If supplied data, features, or samples are incorrect classes. TypeError \u2013 If data.shape[0] != features.shape[0] or data.shape[1] != samples.shape[0]. Examples: Constructing SummarizedPy object from numpy array and pandas DataFrame. >>> import pandas as pd >>> import numpy as np >>> import depy as dp >>> data = np . array ([[ 1 , 2 , 3 ], >>> [ 4 , 5 , 6 ], >>> [ 7 , 8 , 9 ]]) >>> features = pd . DataFrame ({ \"proteinID\" : [ \"feature1\" , \"feature2\" , \"feature3\" ]}) >>> samples = pd . DataFrame ({ \"sample\" : [ \"sample1\" , \"sample2\" , \"sample3\" ]}) >>> sp = dp . SummarizedPy ( data = data , features = features , samples = samples ) <SummarizedPy(data=ndarray(shape=(3, 3), dtype=int64), features=DataFrame(shape=(3, 1)), samples=DataFrame(shape=(3, 1)))> filter_features ( expr = None , mask = None ) \u00b6 Filter SummarizedPy object based on feature metadata, using either Pandas-like query strings or a mask. Parameters: expr ( str , default: None ) \u2013 A Pandas-style query string that can be interpreted by pd.obj.query(expr=expr). mask ( str , default: None ) \u2013 A boolean mask for subsetting. Returns: SummarizedPy \u2013 A filtered SummarizedPy object. Raises: ValueError \u2013 If no valid expr or mask argument is supplied. Examples: Filter out reverse hits in example dataset PXD000438. >>> import depy as dp >>> import re >>> sp = dp . SummarizedPy () . load_example_data () >>> rev_hits = sp . features [ \"protein_id\" ] . apply ( lambda x : bool ( re . match ( \"REV\" , x ))) >>> sp . features [ \"rev\" ] = rev_hits >>> sp = sp . filter_features ( expr = \"~rev\" ) >>> sp = sp . filter_features ( mask =~ rev_hits ) Or using rev_hits as a boolean mask filter_missingness ( frac = 0.75 , strategy = 'all_conditions' , condition_column = None ) \u00b6 Filter SummarizedPy object based on % feature missingness across one of: overall, all conditions, or any condition. Parameters: frac ( float , default: 0.75 ) \u2013 Minimum percentage valid values. Features with missingness greater than or equal to (1 - frac) will be excluded. strategy ( ( overall , all_conditions , any_condition ) , default: 'overall' ) \u2013 Filtering strategy: 'overall' : Require >= frac valid values across all samples. 'all_conditions' : Require >= frac valid values in each condition defined by condition_column . 'any_condition' : Require >= frac valid values in at least one condition defined by condition_column . condition_column ( str , default: None ) \u2013 Name of column in the samples attribute on which to base filtering, in case of 'all_conditions' or 'any_condition'. Returns: SummarizedPy \u2013 A filtered SummarizedPy object. Raises: ValueError \u2013 If an invalid strategy is supplied or if condition_column is required but missing. Examples: Filter out missing values in example dataset PXD000438. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . filter_missingness ( strategy = \"overall\" , frac = 0.75 ) >>> sp = sp . filter_missingness ( strategy = \"any_condition\" , condition_column = \"condition\" , frac = 0.75 ) >>> sp = sp . filter_missingness ( strategy = \"all_conditions\" , condition_column = \"condition\" , frac = 0.75 ) filter_samples ( expr = None , mask = None ) \u00b6 Filter SummarizedPy object based on sample metadata, using either Pandas-like query strings or a mask. Parameters: expr ( str , default: None ) \u2013 A Pandas-style query string that can be interpreted by pd.obj.query(expr=expr). mask ( bool , default: None ) \u2013 A boolean mask for subsetting. Returns: SummarizedPy \u2013 A filtered SummarizedPy object. Raises: ValueError \u2013 If no valid expr or mask argument is supplied. Examples: Filter for ADC samples in example dataset PXD000438. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp . samples [ \"condition\" ] = [ \"ADC\" ] * 6 + [ \"SCC\" ] * 6 >>> sp = sp . filter_samples ( expr = \"condition=='ADC'\" ) import_from_delim_file ( path , delim , data_selector = None , feature_selector = None , replace_val_with_nan = None , clean_column_names = False ) classmethod \u00b6 Alternative constructor from file. Import method that reads data directly from delimited file, including feature data, feature metadata, and sample metadata, and assigns them to data, features, and samples attributes automatically. This is intended for convenient import of standardized outputs like MaxQuant's proteingroups.txt or FragPipe/DIA-NN's diann-output.pg_matrix.tsv. Method uses ColumnSelector objects to assign columns to their relevant storage containers. If no ColumnSelector is provided, the function defaults to assigning all numerical (float64) columns to data and all string (object) columns to feature data. Thus, it is best to explicitly state which columns to import. The samples attribute is automatically populated with the column names from data. The original data row and columns indices are stored in features and samples 'orig_index' variables, resp., for bookkeeping. The read path and delimiter used will be appended to the history attribute. Values in data can be replaced with NaN to indicate missingness (e.g. intensity values of 0). Column names can be automatically cleaned. Parameters: path ( str ) \u2013 Path to file to read in. delim ( str ) \u2013 Delimiter to parse file (e.g. ' ' for .txt or ',' for .csv). data_selector ( ColumnSelector , default: None ) \u2013 A ColumnSelector object with specified names or regex patterns to extract data columns in file. If not specified, defaults to all 'number' dtype columns. feature_selector ( ColumnSelector , default: None ) \u2013 A ColumnSelector object with specified names or regex patterns to extract feature metadata columns in file. If not specified, defaults to all object dtype columns. replace_val_with_nan ( float , default: None ) \u2013 A float numeric value in data to replace with np.nan to indicate missingness (e.g. 0). clean_column_names ( bool , default: False ) \u2013 Whether to clean column names in file before processing. Note that column selection happens on the cleaned column names! Thus, you have to account for this when instantiating the ColumnSelector object. Cleaning will coerce all string to lower case; spaces and hyphens will be replaced with underscores, and leading and trailing whitespace will be trimmed. Returns: SummarizedPy \u2013 A SummarizedPy object. Examples: Read in data from protein groups file (e.g. MQ or FragPipe) and construct SummarizedPy object. Default to placing all numerical columns in 'data', assocaited column names in 'samples', and object or string type columns in 'features'. >>> import depy as dp >>> pg_path = \"~/path/to/my/proteingroups.txt\" >>> sp = dp . SummarizedPy () . import_from_delim_file ( path = pg_path , delim = ' ' , replace_val_with_nan = 0. , clean_column_names = True ) Select columns to import using ColumnSelector object. Assume data are in columns containing sub-string 'LFQ_intensity_'. >>> import depy as dp >>> pg_path = \"~/path/to/my/proteingroups.txt\" >>> data = dp . ColumnSelector ( regex = \"LFQ_intensity_\" ) >>> features = dp . ColumnSelector ( names = [ \"proteinID\" , \"geneSymbol\" , \"proteinDescription\" ]) >>> sp = dp . SummarizedPy () . import_from_delim_file ( path = pg_path , delim = ' ' , data_selector = data , feature_selector = features ) impute_missing_values ( method = None , extra_args = None ) \u00b6 Impute missing values using the ImputeLCMD R package. Several common methods are available under the assumptions of: - MAR (KNN, SVD, MLE) - MNAR (QRILC, MinDet, MinProb) - Both MAR and MNAR (Hybrid) Refer to the ImputeLCMD package documentation for further information: https://cran.r-project.org/web/packages/imputeLCMD/imputeLCMD.pdf Parameters: method ( ( Hybrid , KNN , SVD , MLE , QRILC , MinDet , MinProb ) , default: 'Hybrid' ) \u2013 Imputation method to apply: - 'Hybrid' : Uses an empirical approach (quantile regression) to find a threshold below which missing values are imputed according to MNAR and above which values are imputed according to MAR. Defaults to mar='KNN' and mnar='QRILC'. - 'KNN' : Uses K-nearest neighbors to impute missing values under a MAR assumption. Defaults to k=15 neighbors. - 'SVD' : Uses Singular Value Decomposition to impute missing values under a MAR assumption. Defaults to k=2 principal components. - 'MLE' : Uses Maximum Likelihood Estimation (EM algorithm) to impute missing values under a MAR assumption. - 'QRILC' : Uses Quantile Regression for Imputation of Left Censored Data to impute missing values under an MNAR assumption. Defaults to tune_sigma=1 (SD of the MNAR distribution). - 'MinDet' : Uses imputation by minimum detected value under an MNAR assumption. Defaults to q=0.01 (quantile for minimum value estimation). - 'MinProb' : Uses imputation by random draws from a Gaussian distribution centered on the minimum value. Defaults to q=0.01 and tune_sigma=1. extra_args ( dict , default: None ) \u2013 Used in conjunction with methods that take additional parameters. Valid key-value pairs include: 'mar' : {'KNN', 'SVD', 'MLE'} with method='Hybrid' . 'mnar' : {'QRILC', 'MinDet', 'MinProb'} with `method='Hybrid' . 'k' : int Number of neighbors (KNN) or principal components (SVD). 'q' : float Quantile to estimate minimum value (MinDet, MinProb). 'tune_sigma' : float SD of the MNAR distribution (QRILC, MinProb). Returns: SummarizedPy \u2013 A SummarizedPy object with imputed missing values. Raises: ValueError \u2013 If an invalid value is supplied to 'method'. Examples: Impute missing values using ImputeLCMD's hybrid strategy. Use example dataset PXD000438 after filtering excessive missingness. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . filter_missingness ( strategy = \"overall\" ) >>> sp = sp . impute_missing_values ( method = \"Hybrid\" ) limma_trend_dea ( design_formula = None , contrasts = None , feature_id_col = None , robust = False , block = None , array_weights = False , extra_args = None ) \u00b6 Run differential expression analysis (DEA) with limma-trend. Limma powers its analyses by incorporating an empirical mean-variance trend estimated from the data as a prior. This alleviates the issue of estimating fold changes in the face of heteroscedasticity. In short, low-abundant features are prone to false positives due to inherently lower variance, whereas the opposite is true for high-abundant features, which are prone to false negatives. By modeling the overall mean-variance trend in the data and incorporating it as prior, information is shared across samples (which powers low-N designs) and features are effectively regularized. Compared to traditional parametric statistics, this Bayesian approach has consistently been found to be more powerful and achieve better FDR (false discovery rate) control. Additionally, a robust approximation can be used if the data contain hypo-/hypervariable features to avoid skewing the mean-variance trend. By fundamentally utilizing linear models, limma can accommodate complex designs, including fixed and random factors (i.e. mixed effects, such as nested factors or repeated measures) and their combination (i.e. to model between- and within-subjects designs). Limma can also incorporate sample quality weights, which are extremely powerful, especially in noisy datasets, which is often the case with human or animal samples. Using limma's arrayWeights function, samples are up- or down-weighted based on how variable they are compared to the average sample. Importantly, this function takes the experimental design into account when estimating the sample weights. Moreover, the user can provide an arbitrary design or none at all to estimate averaged weights for different groups of samples (i.e. in cases where sample quality is known to be especially poor according to some condition or technical covariate) or simply estimate sample-specific weights independent of design covariates. arrayWeights is run with the 'REML' method that allows for missing values. The weights are inversely proportional to sample variability (i.e. a weight of 0.5 is twice as variable as the average sample; weights >1 are less variable than the average and tend to reflect higher quality). The weights can be stabilized further and squeezed towards 1 by increasing the 'prior_n' parameter >10 (default); this tends to make weights more symmetric around 1 (average/equality), thus up- and down-weighting samples by similar magnitudes, rather disproportionately up-weighting good samples. Parameters: design_formula ( str , default: None ) \u2013 A formula describing the linear model. Covariates must be present in samples attribute. Must begin with a tilde (~) and add covariates with '+'. Note: formula may not contain intercept term contrasts ( dict , default: None ) \u2013 A dictionary containing contrast labels (keys) and contrast definitions (values). Contrasts are defined by adding or subtracting levels of the covariates included in the design formula. Additional scaling factors are allowed, such as dividing by the number of included terms to get the average. feature_id_col ( str , default: None ) \u2013 The name of a column in the features attribute to name features by. If None, the method defaults to naming features according to their index. robust ( bool , default: False ) \u2013 Whether to run limma-trend with robust approximation. block ( str , default: None ) \u2013 Name of a column in samples attribute to using as a blocking variable. This must be used if running a model with both between- and within-subjects factors. The blocking variable should correspond to the column (subject) that gave rise to repeated values. array_weights ( bool , default: False ) \u2013 Whether to estimate sample quality weights. extra_args ( dict , default: None ) \u2013 Used in conjunction with array_weights to specify additional arguments. Valid key-value pairs include: prior_n : int, The number of prior features to add (defaults to 10) to increase squeezing toward 1. var_group : str, Name of a column in samples indicating groups (levels) that should be assigned different average weights. sample_id_col : str, A column in samples attribute to use for sample labelling. This makes reading the sample weights output in limma_trend_dea.log easier. Note that names must be unique and may not start with numbers. If None, defaults to naming samples according to their index. Returns: SummarizedPy \u2013 A SummarizedPy object with a results attribute containing limma-trend DEA results with columns: contrast_label : name of the contrast contrast : contrast definition feature : feature name logfc : log2 fold change (i.e. regression coefficient) ci_l : lower confidence interval for logfc ci_r : upper confidence interval for logfc aveexpr : average feature expression level t : t-value for the associated test p_value : nominal p-value for the associated test adj_p_val : Benjamini-Hochberg-based false discovery rate b : log-odds of differential expression Raises: ValueError \u2013 If no design_formula or contrasts arguments are provided. Examples: Full DEA pipeline on example dataset PXD000438. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp . samples [ \"condition\" ] = [ \"ADC\" ] * 6 + [ \"SCC\" ] * 6 # Add condition variable >>> sp = sp . filter_missingness ( strategy = \"overall\" ) # Pre-process >>> sp = sp . transform_features ( method = \"log\" , by = 2 ) >>> sp = sp . impute_missing_values ( method = \"Hybrid\" ) >>> sp = sp . surrogate_variable_analysis ( mod = \"~condition\" ) >>> des = \"~condition+sv_1+sv_2+sv_3\" # design formula (incl. 'condition' and surrogate variables) >>> contr = { \"SCCvsADC\" : \"SCC-ADC\" } # define contrast (levels must be present in covariates above) >>> sp = sp . limma_trend_dea ( design_formula = des , contrasts = contr , array_weights = True ) # with array_weights option >>> sp . results # Check newly created results attribute load_example_data () classmethod \u00b6 Load a real-world example proteomics dataset for demonstration purposes. The function loads dataset 'PXD000438' from the ImputeLCMD package. The data were generated from a super-SILAC experiment of human adenocacinoma (ADC) and squamous cell carcinoma (SCC) samples. The dataset contains six ADC and six SCC samples and 3,709 proteomic features with raw feature intensities and missing values. Samples 092.1-3 and 441.1-3 are ADC and 561.1-3 and 691.1-3 are SCC. For more information about the dataset: https://proteomecentral.proteomexchange.org/cgi/GetDataset?ID=PXD000438 Returns: SummarizedPy \u2013 A SummarizedPy object with 12 samples and 3,709 features. Examples: Load example dataset. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () <SummarizedPy(data=ndarray(shape=(3709, 12), dtype=float64), features=DataFrame(shape=(3709, 1)), samples=DataFrame(shape=(12, 1)))> load_sp ( path = None ) classmethod \u00b6 Load a previously saved SummarizedPy object, stored as a pickle file on disk. Parameters: path ( str , default: None ) \u2013 Path to stored pickle file. Returns: SummarizedPy \u2013 A SummarizedPy object. Examples: Load a saved SP object from pickle file. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_sp ( \"my_sp.pkl\" ) plot_pca ( standardize = True , n_comp = None , fill_by = None , label = False ) \u00b6 Generate PCA plot of data using the first two principal components. PCA is computed using scikit-learn's PCA estimator with defaults. If data contain features with missing values, these will simply be omitted, as PCA requires complete data. Parameters: standardize ( bool , default: True ) \u2013 Whether to standardize (i.e. feature-wise z-scoring) data before computing PCA. n_comp ( int , default: None ) \u2013 Number of principal components to calculate. Defaults to min(n_samples, n_features) as in sklearn.decimposition.PCA fill_by ( str , default: None ) \u2013 Valid column name in samples attribute to use for coloring. label ( bool , default: False ) \u2013 Whether to label points according sample variable in samples attribute. Returns: tuple \u2013 matplotlib figure and axes objects. fig : .Figure ax : ~matplotlib.axes.Axes Raises: ValueError \u2013 If an invalid value is supplied to any of the arguments. Examples: Generate PCA plot on standardized data. >>> sp . plot_pca ( standardize = True ) save_sp ( path = None ) \u00b6 Save SummarizedPy object to disk using pickle for easy loading in the future. Method automatically appends '.pkl' to file. Parameters: path ( str , default: None ) \u2013 Path to save pickle file. Examples: Save SP object to disk. >>> sp . save_sp ( path = \"my_sp\" ) select_variable_features ( top_n = None , top_percentile = None , plot = False ) \u00b6 Select highly variable features (HVF) based on deviation from data mean-variance trend. Uses LOWESS to fit a smooth trend to the feature-wise mean and standard deviation values. Note: if log2 transformation has not been applied using transform_features(method='log',by=2) it will be applied prior to fitting the mean-variance trend. Data will be returned on the original scale. Parameters: top_n ( int , default: None ) \u2013 Number of top variable features to return. Mutually exclusive with top_percentile . top_percentile ( int or float , default: None ) \u2013 The top Nth percentile (i.e. 100-top_percentile) of variable features to return. Mutually exclusive with top_n . plot ( bool , default: False ) \u2013 Whether to plot the fitted mean-variance trend and highlight HVFs. Returns: SummarizedPy \u2013 A SummarizedPy object. Raises: ValueError \u2013 If no valid top_n or top_percentile arguments are supplied. Examples: Select top 500 most variable features in example dataset PXD000438 and plot the fitted mean-variance trend. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . select_variable_features ( top_n = 500 , plot = True ) surrogate_variable_analysis ( mod = None , mod0 = '~1' , num_sv = None ) \u00b6 Run surrogate variable analysis to estimate latent factors that capture expression heterogeneity or hidden batch effects. The surrogate variables (SVs) will be added to the samples attribute and can be included as covariates in DEA. SVs are estimated through PCA on the residualized feature matrix after regressing out known experimental and technical/batch covariates. This is done by supplying the method with a fully parameterized model (mod), including all known covariates (experimental and technical; i.e. as present in the samples attribute), and a null model, including only technical (adjustment) covariates (mod0). The number of significant surrogate variables to estimate can then be specified using the num_sv argument; alternatively, the method can be run without specifying a number and allowing SVA to estimate the number empirically (using SVA's 'num.sv' function and the default 'leek' method). Note that this can return 0 SVs and fail. However, it is still possible to find significant SVs by forcing the method to run with a pre-specified num_sv argument. The mod and mod0 arguments must be specified using R formula formatting, which all start with a tilde (~) symbol and add covariates (+) and their interactions (*). Covariates must be present in the samples attribute. If no technical covariates are known, the method will run with the recommended default of \"~1\" (i.e. only using an intercept term). For more information, see: https://bioconductor.org/packages/3.19/bioc/vignettes/sva/inst/doc/sva.pdf Parameters: mod ( str , default: None ) \u2013 A formula describing the fully parameterized model (incl. all known covariates). Must begin with a tilde (~) and add covariates with '+'. mod0 ( str , default: '~1' ) \u2013 A formula describing the null model (incl. all known adjustment covariates). Must begin with a tilde (~) and add covariates with '+'. Defaults to '~1'. num_sv ( int , default: None ) \u2013 The number of significant surrogate variables to estimate. Returns: SummarizedPy \u2013 A SummarizedPy object with estimate surrogate variables in the samples attribute. Raises: ValueError \u2013 If no mod formula is supplied. Examples: Use SVA to estimate surrogate variables for inclusion in DEA. Use example dataset PXD000438: filter missing values and log2 transform features first. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . filter_missingness ( strategy = \"overall\" ) # Filter excessive missinginess (this is important) >>> sp = sp . transform_features ( method = \"log\" , by = 2 ) # Log transform data (important) >>> sp = sp . impute_missing_values ( method = \"Hybrid\" ) # Optionally, impute missing remaining values (sva excludes any feature with nan values) >>> sp = sp . surrogate_variable_analysis ( mod = \"~condition\" ) # Default null model: mod0 = '~1' (intercept-only) >>> sp . samples # SVs now in samples attribute transform_features ( method , axis = None , by = None ) \u00b6 transform_features ( method : Literal [ \"center\" ], axis : int , by : Optional [ Literal [ \"mean\" , \"median\" ]] = None , ) -> SummarizedPy transform_features ( method : Literal [ \"log\" ], by : Optional [ int ] = None ) -> SummarizedPy transform_features ( method : Literal [ \"z-score\" ], axis : int , by : None = None ) -> SummarizedPy transform_features ( method : Literal [ 'vsn' ]) -> SummarizedPy Mathematically transform features stored in data attribute using one of: log (base N), center (using mean or median subtraction), or z-score (standardize). Parameters: axis ( int , default: None ) \u2013 An axis to perform the transformation along: 0 = rows (per-feature) 1 = columns (per-sample). method ( ( log , center , z - score , vsn ) , default: 'log' ) \u2013 Mathematical transformation to apply: 'log' : Applies log base N (use by parameter to set base) transformation across entire data array. 'center' : Center data by subtraction (use in conjunction with by parameter). 'z-score' : Standardizes data using z-score transformation (i.e. (x_i-x_mean)/x_std). NB: applies Bessel's N-1 correction to estimate sample standard deviation. vsn : Applies variance stabilizing normalization, as implemented Huber et al. (2004). by ( str or int , default: None ) \u2013 Used in conjunction with method='center' or method='log' . With 'center' : str, One of 'mean' or 'median' to subtract axis mean or median from each cell. With 'log' : int, An integer for the base of the logarithm (defaults to 2). Returns: SummarizedPy \u2013 A transformed SummarizedPy object. Raises: ValueError \u2013 If an invalid value is supplied to either axis, method, or by. Examples: Transform feature data in example dataset PXD000438. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . transform_features ( method = \"log\" , by = 2 ) # Log transformation (base 2) >>> sp = sp . transform_features ( method = \"center\" , by = \"median\" , axis = 1 ) # Center data sample-wise by median >>> sp = sp . transform_features ( method = \"z-score\" , axis = 0 ) # Feature-wise standardization >>> sp = sp . transform_features ( method = \"vsn\" ) # vsn normalization volcano_plot ( contrasts = None , top_n = 3 , de_colors = None ) \u00b6 Generate volcano plots for limma-trend results, highlighting the top N up- and downregulated features. Parameters: contrasts ( list , default: None ) \u2013 A list of strings referring to the name of contrasts to plot (i.e. column 'contrast_label' in results attribute). Defaults to all contrasts. top_n ( int , default: 3 ) \u2013 Number of top up- and down-regulated features to highlight, ranked by adjusted p-value (FDR). Note: top_n up- and top_n downregulated features will be displayed, rather top_n in total. Defaults to top 3. de_colors ( dict , default: None ) \u2013 Colors to use for upregulated, downregulated, and non-significant features. Must supply 'Up', 'Down', and 'ns' as keys with associated colors (str). Returns: dict \u2013 Dictionaries of matplotlib figure and axes objects for each contrast. fig : .Figure ax : ~matplotlib.axes.Axes Raises: ValueError \u2013 If an invalid value is supplied to either contrasts or de_colors. Examples: Generate volcano plots after running limma_trend_dea method. >>> sp . volcano_plot () >>> sp . volcano_plot ( contrasts = [ \"SCCvsADC\" ]) ColumnSelector \u00b6 Object for pre-selecting columns when constructing SummarizedPy from file. Parameters: names ( list , default: None ) \u2013 A list of strings matching column names. regex ( str , default: None ) \u2013 A string that can be interpreted as a regular expression by re.search. Note that the search is case-insensitive. Examples: Define columns to import from file. Assume data are in columns labeled \"LFQ_intensity_*\". >>> import depy as dp >>> data = dp . ColumnSelector ( regex = \"LFQ_intensity_\" ) >>> features = dp . ColumnSelector ( names = [ \"proteinID\" , \"geneSymbol\" , \"proteinDescription\" ]) >>> sp = dp . SummarizedPy () . import_from_delim_file ( path = \"my/path/proteingroups.txt\" , delim = ' ' , data_selector = data , feature_selector = features ) select_cols ( df ) \u00b6 Parameters: df ( DataFrame ) \u2013 A Pandas DataFrame object on which to perform column selection. Returns: DataFrame \u2013 A pandas DataFrame with selected columns. Raises: KeyError \u2013 If no valid column names are supplied.","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#depy.summarized_py.SummarizedPy","text":"A class to hold bulk proteomics (and metabolomics) data and process it for differential expression analysis. Parameters: data ( ndarray , default: None ) \u2013 A 2D array with shape=(features, samples) holding numerical intensity data. features ( DataFrame , default: None ) \u2013 A DataFrame holding feature metadata. samples ( DataFrame , default: None ) \u2013 A DataFrame holding sample metadata. Attributes: data ( ndarray ) \u2013 A 2D array with shape=(features, samples) holding numerical intensity data. features ( DataFrame ) \u2013 A DataFrame holding feature metadata. samples ( DataFrame ) \u2013 A DataFrame holding sample metadata. history ( list ) \u2013 A list of strings documenting each valid class module call. results ( DataFrame ) \u2013 A DataFrame holding DEA results generated by limma_trend_dea method. Raises: ValueError \u2013 If supplied data, features, or samples are incorrect classes. TypeError \u2013 If data.shape[0] != features.shape[0] or data.shape[1] != samples.shape[0]. Examples: Constructing SummarizedPy object from numpy array and pandas DataFrame. >>> import pandas as pd >>> import numpy as np >>> import depy as dp >>> data = np . array ([[ 1 , 2 , 3 ], >>> [ 4 , 5 , 6 ], >>> [ 7 , 8 , 9 ]]) >>> features = pd . DataFrame ({ \"proteinID\" : [ \"feature1\" , \"feature2\" , \"feature3\" ]}) >>> samples = pd . DataFrame ({ \"sample\" : [ \"sample1\" , \"sample2\" , \"sample3\" ]}) >>> sp = dp . SummarizedPy ( data = data , features = features , samples = samples ) <SummarizedPy(data=ndarray(shape=(3, 3), dtype=int64), features=DataFrame(shape=(3, 1)), samples=DataFrame(shape=(3, 1)))>","title":"SummarizedPy"},{"location":"api/#depy.summarized_py.SummarizedPy.filter_features","text":"Filter SummarizedPy object based on feature metadata, using either Pandas-like query strings or a mask. Parameters: expr ( str , default: None ) \u2013 A Pandas-style query string that can be interpreted by pd.obj.query(expr=expr). mask ( str , default: None ) \u2013 A boolean mask for subsetting. Returns: SummarizedPy \u2013 A filtered SummarizedPy object. Raises: ValueError \u2013 If no valid expr or mask argument is supplied. Examples: Filter out reverse hits in example dataset PXD000438. >>> import depy as dp >>> import re >>> sp = dp . SummarizedPy () . load_example_data () >>> rev_hits = sp . features [ \"protein_id\" ] . apply ( lambda x : bool ( re . match ( \"REV\" , x ))) >>> sp . features [ \"rev\" ] = rev_hits >>> sp = sp . filter_features ( expr = \"~rev\" ) >>> sp = sp . filter_features ( mask =~ rev_hits ) Or using rev_hits as a boolean mask","title":"filter_features"},{"location":"api/#depy.summarized_py.SummarizedPy.filter_missingness","text":"Filter SummarizedPy object based on % feature missingness across one of: overall, all conditions, or any condition. Parameters: frac ( float , default: 0.75 ) \u2013 Minimum percentage valid values. Features with missingness greater than or equal to (1 - frac) will be excluded. strategy ( ( overall , all_conditions , any_condition ) , default: 'overall' ) \u2013 Filtering strategy: 'overall' : Require >= frac valid values across all samples. 'all_conditions' : Require >= frac valid values in each condition defined by condition_column . 'any_condition' : Require >= frac valid values in at least one condition defined by condition_column . condition_column ( str , default: None ) \u2013 Name of column in the samples attribute on which to base filtering, in case of 'all_conditions' or 'any_condition'. Returns: SummarizedPy \u2013 A filtered SummarizedPy object. Raises: ValueError \u2013 If an invalid strategy is supplied or if condition_column is required but missing. Examples: Filter out missing values in example dataset PXD000438. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . filter_missingness ( strategy = \"overall\" , frac = 0.75 ) >>> sp = sp . filter_missingness ( strategy = \"any_condition\" , condition_column = \"condition\" , frac = 0.75 ) >>> sp = sp . filter_missingness ( strategy = \"all_conditions\" , condition_column = \"condition\" , frac = 0.75 )","title":"filter_missingness"},{"location":"api/#depy.summarized_py.SummarizedPy.filter_samples","text":"Filter SummarizedPy object based on sample metadata, using either Pandas-like query strings or a mask. Parameters: expr ( str , default: None ) \u2013 A Pandas-style query string that can be interpreted by pd.obj.query(expr=expr). mask ( bool , default: None ) \u2013 A boolean mask for subsetting. Returns: SummarizedPy \u2013 A filtered SummarizedPy object. Raises: ValueError \u2013 If no valid expr or mask argument is supplied. Examples: Filter for ADC samples in example dataset PXD000438. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp . samples [ \"condition\" ] = [ \"ADC\" ] * 6 + [ \"SCC\" ] * 6 >>> sp = sp . filter_samples ( expr = \"condition=='ADC'\" )","title":"filter_samples"},{"location":"api/#depy.summarized_py.SummarizedPy.import_from_delim_file","text":"Alternative constructor from file. Import method that reads data directly from delimited file, including feature data, feature metadata, and sample metadata, and assigns them to data, features, and samples attributes automatically. This is intended for convenient import of standardized outputs like MaxQuant's proteingroups.txt or FragPipe/DIA-NN's diann-output.pg_matrix.tsv. Method uses ColumnSelector objects to assign columns to their relevant storage containers. If no ColumnSelector is provided, the function defaults to assigning all numerical (float64) columns to data and all string (object) columns to feature data. Thus, it is best to explicitly state which columns to import. The samples attribute is automatically populated with the column names from data. The original data row and columns indices are stored in features and samples 'orig_index' variables, resp., for bookkeeping. The read path and delimiter used will be appended to the history attribute. Values in data can be replaced with NaN to indicate missingness (e.g. intensity values of 0). Column names can be automatically cleaned. Parameters: path ( str ) \u2013 Path to file to read in. delim ( str ) \u2013 Delimiter to parse file (e.g. ' ' for .txt or ',' for .csv). data_selector ( ColumnSelector , default: None ) \u2013 A ColumnSelector object with specified names or regex patterns to extract data columns in file. If not specified, defaults to all 'number' dtype columns. feature_selector ( ColumnSelector , default: None ) \u2013 A ColumnSelector object with specified names or regex patterns to extract feature metadata columns in file. If not specified, defaults to all object dtype columns. replace_val_with_nan ( float , default: None ) \u2013 A float numeric value in data to replace with np.nan to indicate missingness (e.g. 0). clean_column_names ( bool , default: False ) \u2013 Whether to clean column names in file before processing. Note that column selection happens on the cleaned column names! Thus, you have to account for this when instantiating the ColumnSelector object. Cleaning will coerce all string to lower case; spaces and hyphens will be replaced with underscores, and leading and trailing whitespace will be trimmed. Returns: SummarizedPy \u2013 A SummarizedPy object. Examples: Read in data from protein groups file (e.g. MQ or FragPipe) and construct SummarizedPy object. Default to placing all numerical columns in 'data', assocaited column names in 'samples', and object or string type columns in 'features'. >>> import depy as dp >>> pg_path = \"~/path/to/my/proteingroups.txt\" >>> sp = dp . SummarizedPy () . import_from_delim_file ( path = pg_path , delim = ' ' , replace_val_with_nan = 0. , clean_column_names = True ) Select columns to import using ColumnSelector object. Assume data are in columns containing sub-string 'LFQ_intensity_'. >>> import depy as dp >>> pg_path = \"~/path/to/my/proteingroups.txt\" >>> data = dp . ColumnSelector ( regex = \"LFQ_intensity_\" ) >>> features = dp . ColumnSelector ( names = [ \"proteinID\" , \"geneSymbol\" , \"proteinDescription\" ]) >>> sp = dp . SummarizedPy () . import_from_delim_file ( path = pg_path , delim = ' ' , data_selector = data , feature_selector = features )","title":"import_from_delim_file"},{"location":"api/#depy.summarized_py.SummarizedPy.impute_missing_values","text":"Impute missing values using the ImputeLCMD R package. Several common methods are available under the assumptions of: - MAR (KNN, SVD, MLE) - MNAR (QRILC, MinDet, MinProb) - Both MAR and MNAR (Hybrid) Refer to the ImputeLCMD package documentation for further information: https://cran.r-project.org/web/packages/imputeLCMD/imputeLCMD.pdf Parameters: method ( ( Hybrid , KNN , SVD , MLE , QRILC , MinDet , MinProb ) , default: 'Hybrid' ) \u2013 Imputation method to apply: - 'Hybrid' : Uses an empirical approach (quantile regression) to find a threshold below which missing values are imputed according to MNAR and above which values are imputed according to MAR. Defaults to mar='KNN' and mnar='QRILC'. - 'KNN' : Uses K-nearest neighbors to impute missing values under a MAR assumption. Defaults to k=15 neighbors. - 'SVD' : Uses Singular Value Decomposition to impute missing values under a MAR assumption. Defaults to k=2 principal components. - 'MLE' : Uses Maximum Likelihood Estimation (EM algorithm) to impute missing values under a MAR assumption. - 'QRILC' : Uses Quantile Regression for Imputation of Left Censored Data to impute missing values under an MNAR assumption. Defaults to tune_sigma=1 (SD of the MNAR distribution). - 'MinDet' : Uses imputation by minimum detected value under an MNAR assumption. Defaults to q=0.01 (quantile for minimum value estimation). - 'MinProb' : Uses imputation by random draws from a Gaussian distribution centered on the minimum value. Defaults to q=0.01 and tune_sigma=1. extra_args ( dict , default: None ) \u2013 Used in conjunction with methods that take additional parameters. Valid key-value pairs include: 'mar' : {'KNN', 'SVD', 'MLE'} with method='Hybrid' . 'mnar' : {'QRILC', 'MinDet', 'MinProb'} with `method='Hybrid' . 'k' : int Number of neighbors (KNN) or principal components (SVD). 'q' : float Quantile to estimate minimum value (MinDet, MinProb). 'tune_sigma' : float SD of the MNAR distribution (QRILC, MinProb). Returns: SummarizedPy \u2013 A SummarizedPy object with imputed missing values. Raises: ValueError \u2013 If an invalid value is supplied to 'method'. Examples: Impute missing values using ImputeLCMD's hybrid strategy. Use example dataset PXD000438 after filtering excessive missingness. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . filter_missingness ( strategy = \"overall\" ) >>> sp = sp . impute_missing_values ( method = \"Hybrid\" )","title":"impute_missing_values"},{"location":"api/#depy.summarized_py.SummarizedPy.limma_trend_dea","text":"Run differential expression analysis (DEA) with limma-trend. Limma powers its analyses by incorporating an empirical mean-variance trend estimated from the data as a prior. This alleviates the issue of estimating fold changes in the face of heteroscedasticity. In short, low-abundant features are prone to false positives due to inherently lower variance, whereas the opposite is true for high-abundant features, which are prone to false negatives. By modeling the overall mean-variance trend in the data and incorporating it as prior, information is shared across samples (which powers low-N designs) and features are effectively regularized. Compared to traditional parametric statistics, this Bayesian approach has consistently been found to be more powerful and achieve better FDR (false discovery rate) control. Additionally, a robust approximation can be used if the data contain hypo-/hypervariable features to avoid skewing the mean-variance trend. By fundamentally utilizing linear models, limma can accommodate complex designs, including fixed and random factors (i.e. mixed effects, such as nested factors or repeated measures) and their combination (i.e. to model between- and within-subjects designs). Limma can also incorporate sample quality weights, which are extremely powerful, especially in noisy datasets, which is often the case with human or animal samples. Using limma's arrayWeights function, samples are up- or down-weighted based on how variable they are compared to the average sample. Importantly, this function takes the experimental design into account when estimating the sample weights. Moreover, the user can provide an arbitrary design or none at all to estimate averaged weights for different groups of samples (i.e. in cases where sample quality is known to be especially poor according to some condition or technical covariate) or simply estimate sample-specific weights independent of design covariates. arrayWeights is run with the 'REML' method that allows for missing values. The weights are inversely proportional to sample variability (i.e. a weight of 0.5 is twice as variable as the average sample; weights >1 are less variable than the average and tend to reflect higher quality). The weights can be stabilized further and squeezed towards 1 by increasing the 'prior_n' parameter >10 (default); this tends to make weights more symmetric around 1 (average/equality), thus up- and down-weighting samples by similar magnitudes, rather disproportionately up-weighting good samples. Parameters: design_formula ( str , default: None ) \u2013 A formula describing the linear model. Covariates must be present in samples attribute. Must begin with a tilde (~) and add covariates with '+'. Note: formula may not contain intercept term contrasts ( dict , default: None ) \u2013 A dictionary containing contrast labels (keys) and contrast definitions (values). Contrasts are defined by adding or subtracting levels of the covariates included in the design formula. Additional scaling factors are allowed, such as dividing by the number of included terms to get the average. feature_id_col ( str , default: None ) \u2013 The name of a column in the features attribute to name features by. If None, the method defaults to naming features according to their index. robust ( bool , default: False ) \u2013 Whether to run limma-trend with robust approximation. block ( str , default: None ) \u2013 Name of a column in samples attribute to using as a blocking variable. This must be used if running a model with both between- and within-subjects factors. The blocking variable should correspond to the column (subject) that gave rise to repeated values. array_weights ( bool , default: False ) \u2013 Whether to estimate sample quality weights. extra_args ( dict , default: None ) \u2013 Used in conjunction with array_weights to specify additional arguments. Valid key-value pairs include: prior_n : int, The number of prior features to add (defaults to 10) to increase squeezing toward 1. var_group : str, Name of a column in samples indicating groups (levels) that should be assigned different average weights. sample_id_col : str, A column in samples attribute to use for sample labelling. This makes reading the sample weights output in limma_trend_dea.log easier. Note that names must be unique and may not start with numbers. If None, defaults to naming samples according to their index. Returns: SummarizedPy \u2013 A SummarizedPy object with a results attribute containing limma-trend DEA results with columns: contrast_label : name of the contrast contrast : contrast definition feature : feature name logfc : log2 fold change (i.e. regression coefficient) ci_l : lower confidence interval for logfc ci_r : upper confidence interval for logfc aveexpr : average feature expression level t : t-value for the associated test p_value : nominal p-value for the associated test adj_p_val : Benjamini-Hochberg-based false discovery rate b : log-odds of differential expression Raises: ValueError \u2013 If no design_formula or contrasts arguments are provided. Examples: Full DEA pipeline on example dataset PXD000438. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp . samples [ \"condition\" ] = [ \"ADC\" ] * 6 + [ \"SCC\" ] * 6 # Add condition variable >>> sp = sp . filter_missingness ( strategy = \"overall\" ) # Pre-process >>> sp = sp . transform_features ( method = \"log\" , by = 2 ) >>> sp = sp . impute_missing_values ( method = \"Hybrid\" ) >>> sp = sp . surrogate_variable_analysis ( mod = \"~condition\" ) >>> des = \"~condition+sv_1+sv_2+sv_3\" # design formula (incl. 'condition' and surrogate variables) >>> contr = { \"SCCvsADC\" : \"SCC-ADC\" } # define contrast (levels must be present in covariates above) >>> sp = sp . limma_trend_dea ( design_formula = des , contrasts = contr , array_weights = True ) # with array_weights option >>> sp . results # Check newly created results attribute","title":"limma_trend_dea"},{"location":"api/#depy.summarized_py.SummarizedPy.load_example_data","text":"Load a real-world example proteomics dataset for demonstration purposes. The function loads dataset 'PXD000438' from the ImputeLCMD package. The data were generated from a super-SILAC experiment of human adenocacinoma (ADC) and squamous cell carcinoma (SCC) samples. The dataset contains six ADC and six SCC samples and 3,709 proteomic features with raw feature intensities and missing values. Samples 092.1-3 and 441.1-3 are ADC and 561.1-3 and 691.1-3 are SCC. For more information about the dataset: https://proteomecentral.proteomexchange.org/cgi/GetDataset?ID=PXD000438 Returns: SummarizedPy \u2013 A SummarizedPy object with 12 samples and 3,709 features. Examples: Load example dataset. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () <SummarizedPy(data=ndarray(shape=(3709, 12), dtype=float64), features=DataFrame(shape=(3709, 1)), samples=DataFrame(shape=(12, 1)))>","title":"load_example_data"},{"location":"api/#depy.summarized_py.SummarizedPy.load_sp","text":"Load a previously saved SummarizedPy object, stored as a pickle file on disk. Parameters: path ( str , default: None ) \u2013 Path to stored pickle file. Returns: SummarizedPy \u2013 A SummarizedPy object. Examples: Load a saved SP object from pickle file. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_sp ( \"my_sp.pkl\" )","title":"load_sp"},{"location":"api/#depy.summarized_py.SummarizedPy.plot_pca","text":"Generate PCA plot of data using the first two principal components. PCA is computed using scikit-learn's PCA estimator with defaults. If data contain features with missing values, these will simply be omitted, as PCA requires complete data. Parameters: standardize ( bool , default: True ) \u2013 Whether to standardize (i.e. feature-wise z-scoring) data before computing PCA. n_comp ( int , default: None ) \u2013 Number of principal components to calculate. Defaults to min(n_samples, n_features) as in sklearn.decimposition.PCA fill_by ( str , default: None ) \u2013 Valid column name in samples attribute to use for coloring. label ( bool , default: False ) \u2013 Whether to label points according sample variable in samples attribute. Returns: tuple \u2013 matplotlib figure and axes objects. fig : .Figure ax : ~matplotlib.axes.Axes Raises: ValueError \u2013 If an invalid value is supplied to any of the arguments. Examples: Generate PCA plot on standardized data. >>> sp . plot_pca ( standardize = True )","title":"plot_pca"},{"location":"api/#depy.summarized_py.SummarizedPy.save_sp","text":"Save SummarizedPy object to disk using pickle for easy loading in the future. Method automatically appends '.pkl' to file. Parameters: path ( str , default: None ) \u2013 Path to save pickle file. Examples: Save SP object to disk. >>> sp . save_sp ( path = \"my_sp\" )","title":"save_sp"},{"location":"api/#depy.summarized_py.SummarizedPy.select_variable_features","text":"Select highly variable features (HVF) based on deviation from data mean-variance trend. Uses LOWESS to fit a smooth trend to the feature-wise mean and standard deviation values. Note: if log2 transformation has not been applied using transform_features(method='log',by=2) it will be applied prior to fitting the mean-variance trend. Data will be returned on the original scale. Parameters: top_n ( int , default: None ) \u2013 Number of top variable features to return. Mutually exclusive with top_percentile . top_percentile ( int or float , default: None ) \u2013 The top Nth percentile (i.e. 100-top_percentile) of variable features to return. Mutually exclusive with top_n . plot ( bool , default: False ) \u2013 Whether to plot the fitted mean-variance trend and highlight HVFs. Returns: SummarizedPy \u2013 A SummarizedPy object. Raises: ValueError \u2013 If no valid top_n or top_percentile arguments are supplied. Examples: Select top 500 most variable features in example dataset PXD000438 and plot the fitted mean-variance trend. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . select_variable_features ( top_n = 500 , plot = True )","title":"select_variable_features"},{"location":"api/#depy.summarized_py.SummarizedPy.surrogate_variable_analysis","text":"Run surrogate variable analysis to estimate latent factors that capture expression heterogeneity or hidden batch effects. The surrogate variables (SVs) will be added to the samples attribute and can be included as covariates in DEA. SVs are estimated through PCA on the residualized feature matrix after regressing out known experimental and technical/batch covariates. This is done by supplying the method with a fully parameterized model (mod), including all known covariates (experimental and technical; i.e. as present in the samples attribute), and a null model, including only technical (adjustment) covariates (mod0). The number of significant surrogate variables to estimate can then be specified using the num_sv argument; alternatively, the method can be run without specifying a number and allowing SVA to estimate the number empirically (using SVA's 'num.sv' function and the default 'leek' method). Note that this can return 0 SVs and fail. However, it is still possible to find significant SVs by forcing the method to run with a pre-specified num_sv argument. The mod and mod0 arguments must be specified using R formula formatting, which all start with a tilde (~) symbol and add covariates (+) and their interactions (*). Covariates must be present in the samples attribute. If no technical covariates are known, the method will run with the recommended default of \"~1\" (i.e. only using an intercept term). For more information, see: https://bioconductor.org/packages/3.19/bioc/vignettes/sva/inst/doc/sva.pdf Parameters: mod ( str , default: None ) \u2013 A formula describing the fully parameterized model (incl. all known covariates). Must begin with a tilde (~) and add covariates with '+'. mod0 ( str , default: '~1' ) \u2013 A formula describing the null model (incl. all known adjustment covariates). Must begin with a tilde (~) and add covariates with '+'. Defaults to '~1'. num_sv ( int , default: None ) \u2013 The number of significant surrogate variables to estimate. Returns: SummarizedPy \u2013 A SummarizedPy object with estimate surrogate variables in the samples attribute. Raises: ValueError \u2013 If no mod formula is supplied. Examples: Use SVA to estimate surrogate variables for inclusion in DEA. Use example dataset PXD000438: filter missing values and log2 transform features first. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . filter_missingness ( strategy = \"overall\" ) # Filter excessive missinginess (this is important) >>> sp = sp . transform_features ( method = \"log\" , by = 2 ) # Log transform data (important) >>> sp = sp . impute_missing_values ( method = \"Hybrid\" ) # Optionally, impute missing remaining values (sva excludes any feature with nan values) >>> sp = sp . surrogate_variable_analysis ( mod = \"~condition\" ) # Default null model: mod0 = '~1' (intercept-only) >>> sp . samples # SVs now in samples attribute","title":"surrogate_variable_analysis"},{"location":"api/#depy.summarized_py.SummarizedPy.transform_features","text":"transform_features ( method : Literal [ \"center\" ], axis : int , by : Optional [ Literal [ \"mean\" , \"median\" ]] = None , ) -> SummarizedPy transform_features ( method : Literal [ \"log\" ], by : Optional [ int ] = None ) -> SummarizedPy transform_features ( method : Literal [ \"z-score\" ], axis : int , by : None = None ) -> SummarizedPy transform_features ( method : Literal [ 'vsn' ]) -> SummarizedPy Mathematically transform features stored in data attribute using one of: log (base N), center (using mean or median subtraction), or z-score (standardize). Parameters: axis ( int , default: None ) \u2013 An axis to perform the transformation along: 0 = rows (per-feature) 1 = columns (per-sample). method ( ( log , center , z - score , vsn ) , default: 'log' ) \u2013 Mathematical transformation to apply: 'log' : Applies log base N (use by parameter to set base) transformation across entire data array. 'center' : Center data by subtraction (use in conjunction with by parameter). 'z-score' : Standardizes data using z-score transformation (i.e. (x_i-x_mean)/x_std). NB: applies Bessel's N-1 correction to estimate sample standard deviation. vsn : Applies variance stabilizing normalization, as implemented Huber et al. (2004). by ( str or int , default: None ) \u2013 Used in conjunction with method='center' or method='log' . With 'center' : str, One of 'mean' or 'median' to subtract axis mean or median from each cell. With 'log' : int, An integer for the base of the logarithm (defaults to 2). Returns: SummarizedPy \u2013 A transformed SummarizedPy object. Raises: ValueError \u2013 If an invalid value is supplied to either axis, method, or by. Examples: Transform feature data in example dataset PXD000438. >>> import depy as dp >>> sp = dp . SummarizedPy () . load_example_data () >>> sp = sp . transform_features ( method = \"log\" , by = 2 ) # Log transformation (base 2) >>> sp = sp . transform_features ( method = \"center\" , by = \"median\" , axis = 1 ) # Center data sample-wise by median >>> sp = sp . transform_features ( method = \"z-score\" , axis = 0 ) # Feature-wise standardization >>> sp = sp . transform_features ( method = \"vsn\" ) # vsn normalization","title":"transform_features"},{"location":"api/#depy.summarized_py.SummarizedPy.volcano_plot","text":"Generate volcano plots for limma-trend results, highlighting the top N up- and downregulated features. Parameters: contrasts ( list , default: None ) \u2013 A list of strings referring to the name of contrasts to plot (i.e. column 'contrast_label' in results attribute). Defaults to all contrasts. top_n ( int , default: 3 ) \u2013 Number of top up- and down-regulated features to highlight, ranked by adjusted p-value (FDR). Note: top_n up- and top_n downregulated features will be displayed, rather top_n in total. Defaults to top 3. de_colors ( dict , default: None ) \u2013 Colors to use for upregulated, downregulated, and non-significant features. Must supply 'Up', 'Down', and 'ns' as keys with associated colors (str). Returns: dict \u2013 Dictionaries of matplotlib figure and axes objects for each contrast. fig : .Figure ax : ~matplotlib.axes.Axes Raises: ValueError \u2013 If an invalid value is supplied to either contrasts or de_colors. Examples: Generate volcano plots after running limma_trend_dea method. >>> sp . volcano_plot () >>> sp . volcano_plot ( contrasts = [ \"SCCvsADC\" ])","title":"volcano_plot"},{"location":"api/#depy.column_selector.ColumnSelector","text":"Object for pre-selecting columns when constructing SummarizedPy from file. Parameters: names ( list , default: None ) \u2013 A list of strings matching column names. regex ( str , default: None ) \u2013 A string that can be interpreted as a regular expression by re.search. Note that the search is case-insensitive. Examples: Define columns to import from file. Assume data are in columns labeled \"LFQ_intensity_*\". >>> import depy as dp >>> data = dp . ColumnSelector ( regex = \"LFQ_intensity_\" ) >>> features = dp . ColumnSelector ( names = [ \"proteinID\" , \"geneSymbol\" , \"proteinDescription\" ]) >>> sp = dp . SummarizedPy () . import_from_delim_file ( path = \"my/path/proteingroups.txt\" , delim = ' ' , data_selector = data , feature_selector = features )","title":"ColumnSelector"},{"location":"api/#depy.column_selector.ColumnSelector.select_cols","text":"Parameters: df ( DataFrame ) \u2013 A Pandas DataFrame object on which to perform column selection. Returns: DataFrame \u2013 A pandas DataFrame with selected columns. Raises: KeyError \u2013 If no valid column names are supplied.","title":"select_cols"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install DEPy and all its dependencies, please clone the DEPy conda environment : conda env create -f environment.yml conda activate depy This is the intended method of installation. You can install the 'summarizedpy' package on its own using pip, but please don't. pip \u00b6 pip install summarizedpy Note that the SummarizedPy package must be run within the 'depy' conda environment or one cloned from it. In short, this is because summarizedpy runs R as a subprocess in a highly defined environment to isolate it from the user's global R environment. This is necessary to ensure that Bioconductor packages load properly.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install DEPy and all its dependencies, please clone the DEPy conda environment : conda env create -f environment.yml conda activate depy This is the intended method of installation. You can install the 'summarizedpy' package on its own using pip, but please don't.","title":"Stable release"},{"location":"installation/#pip","text":"pip install summarizedpy Note that the SummarizedPy package must be run within the 'depy' conda environment or one cloned from it. In short, this is because summarizedpy runs R as a subprocess in a highly defined environment to isolate it from the user's global R environment. This is necessary to ensure that Bioconductor packages load properly.","title":"pip"},{"location":"usage/","text":"Usage \u00b6 Getting started \u00b6 To use DEPy in a project, start the 'depy' conda environment: conda activate depy Then, open a script or a Jupyter Notebook and simply: import depy as dp Example workflow \u00b6 Loading the data \u00b6 Let's load the example dataset that comes with DEPy (courtesy of the ImputeLCMD package). This is a real-world proteomics dataset of human cancer cell lines (3,709 features, 12 samples). Data were processed with MaxQaunt and comes in the form of protein groups and their intensities. import depy as dp sp = dp . SummarizedPy () sp = sp . load_example_data () Exploring the SummarizedPy object \u00b6 Data are stored in three main attributes: - data (numpy ndarray with float or int dtype) - features (pandas DataFrame) - samples (pandas DataFrame) These can be readily accessed # Check expression data sp . data # Check feature metadata sp . features # Check sample metadata sp . samples To check current dimensions, we can simply invoke the object or call print() on it # See current dimensions in 'repr' format sp # Get a user-friendly summary of the entire object print ( sp ) The last statement will reveal another useful attribute, the history attribute. # Check history attribute sp . history This attribute keeps a faithful record of everything you do to the SummarizedPy object, including function calls and parameters. This is incredibly handy for reproducibility. Subsetting and slicing \u00b6 SummarizedPy objects can be subset and sliced just like SummarizedExperiment in R. The objects are indexed as sp[features, samples] Thus, we can: # Get first feature and all samples sp [ 1 , :] # Or equivalently sp [ 1 ] # Get first sample and all features sp [:, 1 ] Note that if you subset your SummarizedPy , it will be reflected in the history attribute: # Subset first feature and all samples sp = sp [ 1 ] # Check history sp . history A note on dimensionality \u00b6 SummarizedPy enforces a 2D constraint on all three main attributes data features samples such that you always get a 2D numpy array when calling sp.data and a full pandas DataFrame when calling sp.features or sp.samples Critically , SummarizedPy enforces the following rules: sp . data . shape [ 0 ] == sp . features . shape [ 0 ] sp . data . shape [ 1 ] == sp . samples . shape [ 0 ] Indeed, if you were to try import numpy as np import pandas as pd data = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) features = pd . DataFrame ({ \"feature_id\" : [ \"feature1\" , \"feature2\" , \"feature3\" ]}) samples = pd . DataFrame ({ \"sample_id\" : [ \"sample1\" , \"sample2\" ]}) sp = dp . SummarizedPy ( data = data , features = features , samples = samples ) You would get a ValueError saying Number of samples (2) does not match number of columns in data (3) This is because SummarizedPy maps samples and features to data by indexing. Thus, order of rows in these attributes is the source of truth. As a consequence, re-assigning data is not possible and will raise an AttributeError import numpy as np import pandas as pd data = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) features = pd . DataFrame ({ \"feature_id\" : [ \"feature1\" , \"feature2\" , \"feature3\" ]}) samples = pd . DataFrame ({ \"sample_id\" : [ \"sample1\" , \"sample2\" , \"sample3\" ]}) sp = dp . SummarizedPy ( data = data , features = features , samples = samples ) # Trying to re-assign .data will raise AttributeError sp . data = data Similarly, you will not be able to re-assign .history or .results (we will see this one later) You can however mutate in-place, but this will not be reflected in history and should therefore be done at your own peril. We like audit trails, right? # Mutate in-place possible but not recommended sp . data [ 1 , 1 ] = 10 Filtering by sample and feature metadata \u00b6 We can filter the entire SummarizedPy object based on variables in the .features and .samples attributes. This is easily done using the filter_features and filter_samples methods, resp. These functions take a pandas query() style expression or a boolean mask. Returning to our load_example_data example, we can filter based on sample, metadata: # Filtering samples # Include sample condition variable (first six = ADC; last six = SCC) sp . samples [ \"condition\" ] = [ \"ADC\" ] * 6 + [ \"SCC\" ] * 6 # Filter for SCC samples using pandas query expression sp = sp . filter_samples ( expr = \"condition=='SCC'\" ) # Check dimensions sp # Check history sp . history Similarly, we can filter based on feature metadata: # Filtering features # Create boolean vector indicating reverse hits import re rev_hits = sp . features [ \"protein_id\" ] . apply ( lambda x : bool ( re . match ( \"REV\" , x ))) # Add as feature metadata sp . features [ \"rev\" ] = rev_hits # Filter out reverse hits using pandas query expression sp = sp . filter_features ( expr = \"~rev\" ) # Alternatively, use a boolean mask sp . filter_features ( mask =~ rev_hits ) # Check dimensions sp # Check history sp . history Missingness filtering \u00b6 In MS-based -omics, missing data is a guarantee. Whether you have DDA or DIA, there will NA (or nan) values. When conducting differential expression analyses, you need sufficient valid values to base your fold changes on. It makes no sense trying to compute a fold change if one group has 90% missing values for some feature (at least not with linear models). We will not go into all the reasons for missingness, when it is and when it is not a problem; rather, we will simply assume that we want some ceiling on missingness. In Perseus (and proteomics more generally), the standard approaches to missingness filtering include: - % valid values across all samples - % valid values in at least one experimental condition - % valid values in every experimental condition DEPy caters to all three and lets you set the threshold, expressed as the fraction of valid (i.e. non-nan) values. Simply set the strategy= to one of - overall (across all samples) - any_condition (at least one condition) - all_conditions (each condition) Note that if you use one of the condition-based methods, you need to set the condition_column parameter to indicate the name of the column in samples to filter on. Finally, set the frac parameter to the fraction of minimum valid values required (default to 0.75; i.e. $\\ge$75% valid value). # Confirm the presence of nan values import numpy as np np . isnan ( sp . data ) . any () # Missingness filtering method # Across all samples (i.e. independent of condition) sp = sp . filter_missingness ( strategy = \"overall\" , frac = 0.75 ) # At least one condition (i.e. as a fraction of either SCC or ADC) sp = sp . filter_missingness ( strategy = \"any_condition\" , condition_column = \"condition\" , frac = 0.75 ) # In each condition (i.e. as a fraction of both SCC and ADC) sp = sp . filter_missingness ( strategy = \"all_conditions\" , condition_column = \"condition\" , frac = 0.75 ) # Check dimensions sp # Check history sp . history Highly variable feature selection \u00b6 Sometimes, you have too many features to analyze. This can reduce your power in DEA due to the multiple comparisons problem. In machine learning, you may want to start building a regularized model with a smaller subset of features so as not to waste computational time on low-variance features. DEPy has a built-in solution for this with its select_variable_features method. When selecting high-variance features, it is important to account for the heteroscedasticity in the data. Otherwise, we would end up biasing our selection, as the underlying feature variance increases as a function of the average intensity. Similar to the approach taken by Seurat , DEPy models the mean-variance trend of the data by fitting a LOWESS model to the feature-wise means and standard deviations. Note that this calculation will be done on log transformed data. DEPy detects whether a log transformation has been done previously based on the object's history attribute and runs one if not (the data will be returned un-transformed). It then computes standardized residuals (i.e. the deviation from the fitted dispersion values) and ranks the features based on these z-scores. You can then choose to return either the top_n (e.g. top 100) or top_percentile (e.g. top 5th percentile) of variable features. Additionally, the method can display a plot of the data's mean-variance relationship, the fitted LOWESS trend, and highly variable features (HVF) labeled. # Return the top 100 most variable features and show mean-variance trend plot sp = sp . select_variable_features ( top_n = 100 , plot = True ) # Return the top 5% most variable features (calculated as 100-top_percentile) sp = sp . select_variable_features ( top_percentile = 5 , plot = True ) # Check history sp . history Transformations \u00b6 There are several common data transformations and normalizations procedures in proteomics and metabolomics, each of which can be performed in a sample- or feature-wise manner. - log transformation (partly addresses the inherent heteroscedasticity in intensity data) - centering (subtracting a constant such as mean or median to remove offset differences) - standardization (aka z-scoring; enforces unit variance and symmetry about the mean, i.e. 0) Another powerful transformation that addresses both heteroscedasticity and can reduce intra-group variance is the variance stabilizing normalization , popularized by the R package vsn . In short, it uses a generalized logarithm with a linear transformation to remove the asymptote at 0 that otherwise occurs with the standard log transform. The affine (linear) transformation performs a column-wise centering and scaling with parameters estimated empirically. This method, initially introduced for microarray studies, has its roots in error models developed for gas chromatography. Check out the original paper by Wolfgang Huber et al. I personally use this a lot for untargeted metabolomics and with other tools such as MOFA . While many more transformations exist, the three listed ones are arguably the most common in proteomics. DEPy gives you the ability to perform each one in a column- or row-wise manner using the transform_features method. It uses numpy to make them blazingly fast where possible, and calls R for vsn. The three main parameters are: - method: one of 'log' , 'center' , 'z-score' , 'vsn' - by: one of 'mean' or 'median' for method='center' or an int for method='log' - axis: 0 for row-wise (feature-wise), 1 column-wise (sample-wise) (omitted for methods log and vsn ) # Log transformation (base 2) sp = sp . transform_features ( method = \"log\" , by = 2 ) # Center data sample-wise by median sp = sp . transform_features ( method = \"center\" , by = \"median\" , axis = 1 ) # Feature-wise standardization sp = sp . transform_features ( method = \"z-score\" , axis = 0 ) # vsn normalization sp = sp . transform_features ( method = \"vsn\" ) # Check data sp . data # Check history sp . history Imputation \u00b6 Depending on whom you ask, this is either a valid or questionable idea. We will not debate it here. I would simply note (based on having conducted systematic trials) that you should only impute provided you have a sufficient number of valid values to base the imputation on. What constitutes sufficient is like asking 'how long is a piece of string', but if I had to pick a number, I would say $\\ge$50% valid values per condition at a minimum . For our purposes here, missingness comes in two flavors: - MAR (missing at random): the missingness pattern is randomly distributed and independent of the features - MNAR (missing not at random): usually left-censored data in MS-based -omics, whereby features are missing due to low abundance. DEPy runs the ImputeLCMD R package under the hood. This is a great package, created by Cosmin Lazar et al. It comes with methods for MAR, MNAR, and critically , hybrid MAR-MNAR assumptions. Which assumption to use is beyond the scope of this tutorial, but check out Lazar's excellent paper on the topic. Accordingly, DEPy comes with all of those methods and supports all native parameters. Exploring them all is a bit much, so we will only demonstrate the hybrid method. # Filter excessive missinginess sp = sp . filter_missingness ( strategy = \"overall\" ) # Impute remaining missing values with hybrid approach # MAR = KNN (default) # MNAR = QRILC (default) sp = sp . impute_missing_values ( method = \"Hybrid\" , extra_args = { \"mar\" : \"KNN\" , \"mnar\" : \"QRILC\" }) # Check history sp . history Surrogate variable analysis \u00b6 This is an excellent tool for any bioinformatician to have in their toolbox. Indeed, it has proven invaluable in some of my own work, which deals with real-world, messy data from humans and animals. The method is surprisingly intuitive for all its seeming complexity. Check out the original paper by Leek et al.; it is a fantastic read. Why use sva \u00b6 Sometimes, we have known batch effects or technical covariates that can be readily included in our models as adjustment variables. However, sometimes, there can be hidden batch effects in your data that manifest as latent noise and correlations among features that we cannot account for. How do you include something you cannot measure? This is why sva \u00b6 In a nutshell, surrogate variable analysis (sva) performs PCA on the residuals of the data matrix after regressing out a fully parametrized models that includes all known experimental and technical covariates. Then, it uses the resultant factors to capture latent covariation among features, check which features associates with the factors, computes a new model on the reduced set, and returns the latent factors (aka \"surrogate variables\"), which can be included as technical covariates in your subsequent model. neat... DEPy runs the sva R package with all its native arguments. Simply specify your fully parametrized model, including all variables of interest (both experimental ones you care about, and adjustment/technical/batch variables). Additionally, sva requires a so-called null model that includes only known adjustment variables. If you do not have any known ones, the default is to simply use an intercept model. Note that you must specify the models with R-style tilde expressions; e.g. ~var1+var2 # Filter excessive missinginess (this is important) sp = sp . filter_missingness ( strategy = \"overall\" ) # Log transform data (important) sp = sp . transform_features ( method = \"log\" , by = 2 ) # Optionally, impute missing remaining values (sva excludes any feature with nan values) sp = sp . impute_missing_values ( method = \"Hybrid\" ) # Run sva # Default null model: mod0 = '~1' (intercept-only) sp = sp . surrogate_variable_analysis ( mod = \"~condition\" ) # Surrogate variables are added to samples attribute sp . samples # Check history sp . history limma-trend \u00b6 Now for the actual differential expression analysis, we will use limma, brain child of Gordon Smyth (check out the legendary paper ). Limma leverages an empirically estimated mean-variance trend as a prior to adjust for the heteroscedasticity common to so many -omics modalities. This Bayesian prior serves as a form shrinkage, which mitigates the inflated false positive and negative rates that come with conducting fold change analysis on low- and high-abundant features if the mean-variance is not accounted for. This is particularly useful for small sample sizes. Limma-trend was implemented and adapted in the proteomics R packages DEqMS and DEP for these very reasons. Limma has been found to be more powerful and exert better FDR control than standard parametric statistics (i.e. t-test and ANOVA). Critically, limma-trend performs about as well when adjusting for the mean-variance trend using the feature-wise intensity distribution as when using peptide or PSM counts to adjust protein-level variance (see DEqMS paper ). Another massive benefit to limma is that it can incorporate sample quality weights calculated by its arrayWeights function. In my years, I have found that it makes all the difference when working with data from human and animal samples. In short, the weights are calculated by allowing each feature to have a sample-specific source of variation. Using the overall mean-variance trend of the dataset, each sample can be quantified in terms of how much it deviates from the average (i.e. how 'noisy' a sample is). This information is then incorporated into the weights, which are reciprocal logarithms of that deviation (e.g. a quality weight of 0.5 is 2x as variable as the average sample; i.e. likely low quality). The weights take the experimental design into account and can be estimated for each sample or averaged as a function of some covariate (e.g. if quality was found to be reliably lower due to variable). Finally, limma is highly flexible due to being a linear model. it can incorporate mixed effects, repeated measures, and between- and within-subjects factors. Limma is available with most of its native parameters. Simply specify the design, the contrasts, whether to include array weights, etc. - design_formula (R-style tilde expression with covariates of interest) - contrasts (dictionary with the names and definitions of contrasts) - array_weights (boolean; whether to include sample quality weights) A brief note on design formulae \u00b6 DEPy will automatically enforce a marginal means model; that is, an intercept will not be included even if you specify one. This is to keep the design matrix tidy to ensure proper matching between its column names and contrast terms. To this end, DEPy will: - inject a '~' if you forget one - add a '0+' to the start of the design_formula In general, the intercept term is often of little interest (it represents the grand average) and will not affect fold changes. # Specify design formula (including 'condition' and surrogate variables) des = \"~condition+sv_1+sv_2+sv_3\" # Define contrast (levels must be present in covariates above) contr = { \"SCCvsADC\" : \"SCC-ADC\" } # Run limma-trend with array_weights option sp = sp . limma_trend_dea ( design_formula = des , contrasts = contr , array_weights = True ) # Check newly created results attribute sp . results # Check history sp . history Volcano plots \u00b6 Finally, we will plot the estimate log fold changes against the log-transformed nominal p-values per feature for each contrast. Better known as a volcano plot. This is very simple in DEPy: you can either provide a list contrast names or let DEPy produce one volcano plot per contrast. Plots are generated with matplotlib and returned as a tuple of dictionaries with keys = contrast names and values = matplotlib.Figure or matplotlib.Axes objects. # Generate volcano plots for all contrasts fig , ax = sp . volcano_plot () # Optionally, specify the name of a contrast sp . volcano_plot ( contrasts = [ \"SCCvsADC\" ]) By default, the function plots the top 3 up- and top 3 down-regulated features according to FDR. This can be changed with the top_n parameter. You can also change color scheme by providing a dictionary with names 'Up' , 'Down' , and 'ns' , and hexcodes or valid color names as values. # Highlight top up- and down-regulated features fig , ax = sp . volcano_plot ( top_n = 1 ) # Change colors de_colors = { \"Up\" : \"red\" , \"Down\" : \"blue\" , \"ns\" : \"white\" } sp . volcano_plot ( de_colors = de_colors ) PCA plots \u00b6 It is common to visualize your data with PCA to get an idea of what the variance structure is like. This is particularly useful in combination with labeling and coloring according to some condition or variable to reveal clustering or outlier samples. DEPy has a simple plotting function for this, which calls scikit-learn's PCA estimator under the hood with all defaults (standard SVD). It is important to remember that PCA is highly sensitive to feature scale and range, such that if some features have greater range, the model will return components that mainly reflect the differences in feature scales. This is commonly remedied by standardizing the features (i.e. z-scoring) first. This can be done by calling the plot_pca method with standardize=True . The method both displays the PCA plot and returns matplotlib.Figure or matplotlib.Axes objects as a tuple. # Plot PCA with example dataset using method defaults fig , ax = sp . plot_pca () You can change the number of principal components to estimate using the n_comp argument. However, the method only plots samples along the first two components. To color samples by some condition or variable, simply provide a valid string to the fill_by argument indicating a column in the samples attribute. To label individual sample points, set label=True . # Plot PCA and color by tumor condition and label individual samples fig , ax = sp . plot_pca ( fill_by = \"condition\" , label = True ) Saving and loading SummarizedPy objects \u00b6 To allow users to save and load SummarizedPy objects complete with history, DEPy leverages Python's pickle library. Fortunately, all data structures in a SummarizedPy are readily serializable and, thus, lend themselves well to easy read/write. Simply call the save_sp and load_sp methods! The former automatically appends the correct \".pkl\" suffix, so there is no need to remember it. # Save to disk sp . save_sp ( \"my_sp\" ) # Load from disk sp = dp . SummarizedPy () . load_sp ( \"my_sp.pkl\" ) There you have it!","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#getting-started","text":"To use DEPy in a project, start the 'depy' conda environment: conda activate depy Then, open a script or a Jupyter Notebook and simply: import depy as dp","title":"Getting started"},{"location":"usage/#example-workflow","text":"","title":"Example workflow"},{"location":"usage/#loading-the-data","text":"Let's load the example dataset that comes with DEPy (courtesy of the ImputeLCMD package). This is a real-world proteomics dataset of human cancer cell lines (3,709 features, 12 samples). Data were processed with MaxQaunt and comes in the form of protein groups and their intensities. import depy as dp sp = dp . SummarizedPy () sp = sp . load_example_data ()","title":"Loading the data"},{"location":"usage/#exploring-the-summarizedpy-object","text":"Data are stored in three main attributes: - data (numpy ndarray with float or int dtype) - features (pandas DataFrame) - samples (pandas DataFrame) These can be readily accessed # Check expression data sp . data # Check feature metadata sp . features # Check sample metadata sp . samples To check current dimensions, we can simply invoke the object or call print() on it # See current dimensions in 'repr' format sp # Get a user-friendly summary of the entire object print ( sp ) The last statement will reveal another useful attribute, the history attribute. # Check history attribute sp . history This attribute keeps a faithful record of everything you do to the SummarizedPy object, including function calls and parameters. This is incredibly handy for reproducibility.","title":"Exploring the SummarizedPy object"},{"location":"usage/#subsetting-and-slicing","text":"SummarizedPy objects can be subset and sliced just like SummarizedExperiment in R. The objects are indexed as sp[features, samples] Thus, we can: # Get first feature and all samples sp [ 1 , :] # Or equivalently sp [ 1 ] # Get first sample and all features sp [:, 1 ] Note that if you subset your SummarizedPy , it will be reflected in the history attribute: # Subset first feature and all samples sp = sp [ 1 ] # Check history sp . history","title":"Subsetting and slicing"},{"location":"usage/#a-note-on-dimensionality","text":"SummarizedPy enforces a 2D constraint on all three main attributes data features samples such that you always get a 2D numpy array when calling sp.data and a full pandas DataFrame when calling sp.features or sp.samples Critically , SummarizedPy enforces the following rules: sp . data . shape [ 0 ] == sp . features . shape [ 0 ] sp . data . shape [ 1 ] == sp . samples . shape [ 0 ] Indeed, if you were to try import numpy as np import pandas as pd data = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) features = pd . DataFrame ({ \"feature_id\" : [ \"feature1\" , \"feature2\" , \"feature3\" ]}) samples = pd . DataFrame ({ \"sample_id\" : [ \"sample1\" , \"sample2\" ]}) sp = dp . SummarizedPy ( data = data , features = features , samples = samples ) You would get a ValueError saying Number of samples (2) does not match number of columns in data (3) This is because SummarizedPy maps samples and features to data by indexing. Thus, order of rows in these attributes is the source of truth. As a consequence, re-assigning data is not possible and will raise an AttributeError import numpy as np import pandas as pd data = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) features = pd . DataFrame ({ \"feature_id\" : [ \"feature1\" , \"feature2\" , \"feature3\" ]}) samples = pd . DataFrame ({ \"sample_id\" : [ \"sample1\" , \"sample2\" , \"sample3\" ]}) sp = dp . SummarizedPy ( data = data , features = features , samples = samples ) # Trying to re-assign .data will raise AttributeError sp . data = data Similarly, you will not be able to re-assign .history or .results (we will see this one later) You can however mutate in-place, but this will not be reflected in history and should therefore be done at your own peril. We like audit trails, right? # Mutate in-place possible but not recommended sp . data [ 1 , 1 ] = 10","title":"A note on dimensionality"},{"location":"usage/#filtering-by-sample-and-feature-metadata","text":"We can filter the entire SummarizedPy object based on variables in the .features and .samples attributes. This is easily done using the filter_features and filter_samples methods, resp. These functions take a pandas query() style expression or a boolean mask. Returning to our load_example_data example, we can filter based on sample, metadata: # Filtering samples # Include sample condition variable (first six = ADC; last six = SCC) sp . samples [ \"condition\" ] = [ \"ADC\" ] * 6 + [ \"SCC\" ] * 6 # Filter for SCC samples using pandas query expression sp = sp . filter_samples ( expr = \"condition=='SCC'\" ) # Check dimensions sp # Check history sp . history Similarly, we can filter based on feature metadata: # Filtering features # Create boolean vector indicating reverse hits import re rev_hits = sp . features [ \"protein_id\" ] . apply ( lambda x : bool ( re . match ( \"REV\" , x ))) # Add as feature metadata sp . features [ \"rev\" ] = rev_hits # Filter out reverse hits using pandas query expression sp = sp . filter_features ( expr = \"~rev\" ) # Alternatively, use a boolean mask sp . filter_features ( mask =~ rev_hits ) # Check dimensions sp # Check history sp . history","title":"Filtering by sample and feature metadata"},{"location":"usage/#missingness-filtering","text":"In MS-based -omics, missing data is a guarantee. Whether you have DDA or DIA, there will NA (or nan) values. When conducting differential expression analyses, you need sufficient valid values to base your fold changes on. It makes no sense trying to compute a fold change if one group has 90% missing values for some feature (at least not with linear models). We will not go into all the reasons for missingness, when it is and when it is not a problem; rather, we will simply assume that we want some ceiling on missingness. In Perseus (and proteomics more generally), the standard approaches to missingness filtering include: - % valid values across all samples - % valid values in at least one experimental condition - % valid values in every experimental condition DEPy caters to all three and lets you set the threshold, expressed as the fraction of valid (i.e. non-nan) values. Simply set the strategy= to one of - overall (across all samples) - any_condition (at least one condition) - all_conditions (each condition) Note that if you use one of the condition-based methods, you need to set the condition_column parameter to indicate the name of the column in samples to filter on. Finally, set the frac parameter to the fraction of minimum valid values required (default to 0.75; i.e. $\\ge$75% valid value). # Confirm the presence of nan values import numpy as np np . isnan ( sp . data ) . any () # Missingness filtering method # Across all samples (i.e. independent of condition) sp = sp . filter_missingness ( strategy = \"overall\" , frac = 0.75 ) # At least one condition (i.e. as a fraction of either SCC or ADC) sp = sp . filter_missingness ( strategy = \"any_condition\" , condition_column = \"condition\" , frac = 0.75 ) # In each condition (i.e. as a fraction of both SCC and ADC) sp = sp . filter_missingness ( strategy = \"all_conditions\" , condition_column = \"condition\" , frac = 0.75 ) # Check dimensions sp # Check history sp . history","title":"Missingness filtering"},{"location":"usage/#highly-variable-feature-selection","text":"Sometimes, you have too many features to analyze. This can reduce your power in DEA due to the multiple comparisons problem. In machine learning, you may want to start building a regularized model with a smaller subset of features so as not to waste computational time on low-variance features. DEPy has a built-in solution for this with its select_variable_features method. When selecting high-variance features, it is important to account for the heteroscedasticity in the data. Otherwise, we would end up biasing our selection, as the underlying feature variance increases as a function of the average intensity. Similar to the approach taken by Seurat , DEPy models the mean-variance trend of the data by fitting a LOWESS model to the feature-wise means and standard deviations. Note that this calculation will be done on log transformed data. DEPy detects whether a log transformation has been done previously based on the object's history attribute and runs one if not (the data will be returned un-transformed). It then computes standardized residuals (i.e. the deviation from the fitted dispersion values) and ranks the features based on these z-scores. You can then choose to return either the top_n (e.g. top 100) or top_percentile (e.g. top 5th percentile) of variable features. Additionally, the method can display a plot of the data's mean-variance relationship, the fitted LOWESS trend, and highly variable features (HVF) labeled. # Return the top 100 most variable features and show mean-variance trend plot sp = sp . select_variable_features ( top_n = 100 , plot = True ) # Return the top 5% most variable features (calculated as 100-top_percentile) sp = sp . select_variable_features ( top_percentile = 5 , plot = True ) # Check history sp . history","title":"Highly variable feature selection"},{"location":"usage/#transformations","text":"There are several common data transformations and normalizations procedures in proteomics and metabolomics, each of which can be performed in a sample- or feature-wise manner. - log transformation (partly addresses the inherent heteroscedasticity in intensity data) - centering (subtracting a constant such as mean or median to remove offset differences) - standardization (aka z-scoring; enforces unit variance and symmetry about the mean, i.e. 0) Another powerful transformation that addresses both heteroscedasticity and can reduce intra-group variance is the variance stabilizing normalization , popularized by the R package vsn . In short, it uses a generalized logarithm with a linear transformation to remove the asymptote at 0 that otherwise occurs with the standard log transform. The affine (linear) transformation performs a column-wise centering and scaling with parameters estimated empirically. This method, initially introduced for microarray studies, has its roots in error models developed for gas chromatography. Check out the original paper by Wolfgang Huber et al. I personally use this a lot for untargeted metabolomics and with other tools such as MOFA . While many more transformations exist, the three listed ones are arguably the most common in proteomics. DEPy gives you the ability to perform each one in a column- or row-wise manner using the transform_features method. It uses numpy to make them blazingly fast where possible, and calls R for vsn. The three main parameters are: - method: one of 'log' , 'center' , 'z-score' , 'vsn' - by: one of 'mean' or 'median' for method='center' or an int for method='log' - axis: 0 for row-wise (feature-wise), 1 column-wise (sample-wise) (omitted for methods log and vsn ) # Log transformation (base 2) sp = sp . transform_features ( method = \"log\" , by = 2 ) # Center data sample-wise by median sp = sp . transform_features ( method = \"center\" , by = \"median\" , axis = 1 ) # Feature-wise standardization sp = sp . transform_features ( method = \"z-score\" , axis = 0 ) # vsn normalization sp = sp . transform_features ( method = \"vsn\" ) # Check data sp . data # Check history sp . history","title":"Transformations"},{"location":"usage/#imputation","text":"Depending on whom you ask, this is either a valid or questionable idea. We will not debate it here. I would simply note (based on having conducted systematic trials) that you should only impute provided you have a sufficient number of valid values to base the imputation on. What constitutes sufficient is like asking 'how long is a piece of string', but if I had to pick a number, I would say $\\ge$50% valid values per condition at a minimum . For our purposes here, missingness comes in two flavors: - MAR (missing at random): the missingness pattern is randomly distributed and independent of the features - MNAR (missing not at random): usually left-censored data in MS-based -omics, whereby features are missing due to low abundance. DEPy runs the ImputeLCMD R package under the hood. This is a great package, created by Cosmin Lazar et al. It comes with methods for MAR, MNAR, and critically , hybrid MAR-MNAR assumptions. Which assumption to use is beyond the scope of this tutorial, but check out Lazar's excellent paper on the topic. Accordingly, DEPy comes with all of those methods and supports all native parameters. Exploring them all is a bit much, so we will only demonstrate the hybrid method. # Filter excessive missinginess sp = sp . filter_missingness ( strategy = \"overall\" ) # Impute remaining missing values with hybrid approach # MAR = KNN (default) # MNAR = QRILC (default) sp = sp . impute_missing_values ( method = \"Hybrid\" , extra_args = { \"mar\" : \"KNN\" , \"mnar\" : \"QRILC\" }) # Check history sp . history","title":"Imputation"},{"location":"usage/#surrogate-variable-analysis","text":"This is an excellent tool for any bioinformatician to have in their toolbox. Indeed, it has proven invaluable in some of my own work, which deals with real-world, messy data from humans and animals. The method is surprisingly intuitive for all its seeming complexity. Check out the original paper by Leek et al.; it is a fantastic read.","title":"Surrogate variable analysis"},{"location":"usage/#why-use-sva","text":"Sometimes, we have known batch effects or technical covariates that can be readily included in our models as adjustment variables. However, sometimes, there can be hidden batch effects in your data that manifest as latent noise and correlations among features that we cannot account for. How do you include something you cannot measure?","title":"Why use sva"},{"location":"usage/#this-is-why-sva","text":"In a nutshell, surrogate variable analysis (sva) performs PCA on the residuals of the data matrix after regressing out a fully parametrized models that includes all known experimental and technical covariates. Then, it uses the resultant factors to capture latent covariation among features, check which features associates with the factors, computes a new model on the reduced set, and returns the latent factors (aka \"surrogate variables\"), which can be included as technical covariates in your subsequent model. neat... DEPy runs the sva R package with all its native arguments. Simply specify your fully parametrized model, including all variables of interest (both experimental ones you care about, and adjustment/technical/batch variables). Additionally, sva requires a so-called null model that includes only known adjustment variables. If you do not have any known ones, the default is to simply use an intercept model. Note that you must specify the models with R-style tilde expressions; e.g. ~var1+var2 # Filter excessive missinginess (this is important) sp = sp . filter_missingness ( strategy = \"overall\" ) # Log transform data (important) sp = sp . transform_features ( method = \"log\" , by = 2 ) # Optionally, impute missing remaining values (sva excludes any feature with nan values) sp = sp . impute_missing_values ( method = \"Hybrid\" ) # Run sva # Default null model: mod0 = '~1' (intercept-only) sp = sp . surrogate_variable_analysis ( mod = \"~condition\" ) # Surrogate variables are added to samples attribute sp . samples # Check history sp . history","title":"This is why sva"},{"location":"usage/#limma-trend","text":"Now for the actual differential expression analysis, we will use limma, brain child of Gordon Smyth (check out the legendary paper ). Limma leverages an empirically estimated mean-variance trend as a prior to adjust for the heteroscedasticity common to so many -omics modalities. This Bayesian prior serves as a form shrinkage, which mitigates the inflated false positive and negative rates that come with conducting fold change analysis on low- and high-abundant features if the mean-variance is not accounted for. This is particularly useful for small sample sizes. Limma-trend was implemented and adapted in the proteomics R packages DEqMS and DEP for these very reasons. Limma has been found to be more powerful and exert better FDR control than standard parametric statistics (i.e. t-test and ANOVA). Critically, limma-trend performs about as well when adjusting for the mean-variance trend using the feature-wise intensity distribution as when using peptide or PSM counts to adjust protein-level variance (see DEqMS paper ). Another massive benefit to limma is that it can incorporate sample quality weights calculated by its arrayWeights function. In my years, I have found that it makes all the difference when working with data from human and animal samples. In short, the weights are calculated by allowing each feature to have a sample-specific source of variation. Using the overall mean-variance trend of the dataset, each sample can be quantified in terms of how much it deviates from the average (i.e. how 'noisy' a sample is). This information is then incorporated into the weights, which are reciprocal logarithms of that deviation (e.g. a quality weight of 0.5 is 2x as variable as the average sample; i.e. likely low quality). The weights take the experimental design into account and can be estimated for each sample or averaged as a function of some covariate (e.g. if quality was found to be reliably lower due to variable). Finally, limma is highly flexible due to being a linear model. it can incorporate mixed effects, repeated measures, and between- and within-subjects factors. Limma is available with most of its native parameters. Simply specify the design, the contrasts, whether to include array weights, etc. - design_formula (R-style tilde expression with covariates of interest) - contrasts (dictionary with the names and definitions of contrasts) - array_weights (boolean; whether to include sample quality weights)","title":"limma-trend"},{"location":"usage/#a-brief-note-on-design-formulae","text":"DEPy will automatically enforce a marginal means model; that is, an intercept will not be included even if you specify one. This is to keep the design matrix tidy to ensure proper matching between its column names and contrast terms. To this end, DEPy will: - inject a '~' if you forget one - add a '0+' to the start of the design_formula In general, the intercept term is often of little interest (it represents the grand average) and will not affect fold changes. # Specify design formula (including 'condition' and surrogate variables) des = \"~condition+sv_1+sv_2+sv_3\" # Define contrast (levels must be present in covariates above) contr = { \"SCCvsADC\" : \"SCC-ADC\" } # Run limma-trend with array_weights option sp = sp . limma_trend_dea ( design_formula = des , contrasts = contr , array_weights = True ) # Check newly created results attribute sp . results # Check history sp . history","title":"A brief note on design formulae"},{"location":"usage/#volcano-plots","text":"Finally, we will plot the estimate log fold changes against the log-transformed nominal p-values per feature for each contrast. Better known as a volcano plot. This is very simple in DEPy: you can either provide a list contrast names or let DEPy produce one volcano plot per contrast. Plots are generated with matplotlib and returned as a tuple of dictionaries with keys = contrast names and values = matplotlib.Figure or matplotlib.Axes objects. # Generate volcano plots for all contrasts fig , ax = sp . volcano_plot () # Optionally, specify the name of a contrast sp . volcano_plot ( contrasts = [ \"SCCvsADC\" ]) By default, the function plots the top 3 up- and top 3 down-regulated features according to FDR. This can be changed with the top_n parameter. You can also change color scheme by providing a dictionary with names 'Up' , 'Down' , and 'ns' , and hexcodes or valid color names as values. # Highlight top up- and down-regulated features fig , ax = sp . volcano_plot ( top_n = 1 ) # Change colors de_colors = { \"Up\" : \"red\" , \"Down\" : \"blue\" , \"ns\" : \"white\" } sp . volcano_plot ( de_colors = de_colors )","title":"Volcano plots"},{"location":"usage/#pca-plots","text":"It is common to visualize your data with PCA to get an idea of what the variance structure is like. This is particularly useful in combination with labeling and coloring according to some condition or variable to reveal clustering or outlier samples. DEPy has a simple plotting function for this, which calls scikit-learn's PCA estimator under the hood with all defaults (standard SVD). It is important to remember that PCA is highly sensitive to feature scale and range, such that if some features have greater range, the model will return components that mainly reflect the differences in feature scales. This is commonly remedied by standardizing the features (i.e. z-scoring) first. This can be done by calling the plot_pca method with standardize=True . The method both displays the PCA plot and returns matplotlib.Figure or matplotlib.Axes objects as a tuple. # Plot PCA with example dataset using method defaults fig , ax = sp . plot_pca () You can change the number of principal components to estimate using the n_comp argument. However, the method only plots samples along the first two components. To color samples by some condition or variable, simply provide a valid string to the fill_by argument indicating a column in the samples attribute. To label individual sample points, set label=True . # Plot PCA and color by tumor condition and label individual samples fig , ax = sp . plot_pca ( fill_by = \"condition\" , label = True )","title":"PCA plots"},{"location":"usage/#saving-and-loading-summarizedpy-objects","text":"To allow users to save and load SummarizedPy objects complete with history, DEPy leverages Python's pickle library. Fortunately, all data structures in a SummarizedPy are readily serializable and, thus, lend themselves well to easy read/write. Simply call the save_sp and load_sp methods! The former automatically appends the correct \".pkl\" suffix, so there is no need to remember it. # Save to disk sp . save_sp ( \"my_sp\" ) # Load from disk sp = dp . SummarizedPy () . load_sp ( \"my_sp.pkl\" ) There you have it!","title":"Saving and loading SummarizedPy objects"}]}